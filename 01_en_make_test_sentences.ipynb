{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import textacy.datasets\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "cw = textacy.datasets.CapitolWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MQNLI words to filter out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mqnli_transitive_verbs.txt', 'r') as f:\n",
    "    mqnli_verbs = f.readlines()\n",
    "mqnli_verbs = [x.split()[2].strip('\\n') for x in mqnli_verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mqnli_agents.txt', 'r') as f:\n",
    "    mqnli_agents = f.readlines()\n",
    "mqnli_agents = [x.strip('\\n') for x in mqnli_agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mqnli_things.txt', 'r') as f:\n",
    "    mqnli_things = f.readlines()\n",
    "mqnli_things = [x.strip('\\n') for x in mqnli_things]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mqnli_nouns = mqnli_agents + mqnli_things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlemma_cnt = defaultdict(int)\n",
    "vlemma_transitive_cnt = defaultdict(int)\n",
    "vlemma2past = defaultdict(Counter)\n",
    "nounlemma_cnt = defaultdict(int)\n",
    "nounlemma2pl = defaultdict(Counter)\n",
    "\n",
    "for text,record in cw.records():\n",
    "    processed = nlp(text)\n",
    "    for token in processed:\n",
    "        if token.pos_ == 'NOUN' and 'Number=Plur' in token.morph and re.match(r'^[A-Za-z]*$',token.text):\n",
    "            nounlemma_cnt[token.lemma_] += 1\n",
    "            nounlemma2pl[token.lemma_].update([token.text.lower(),])\n",
    "        if token.pos_ == 'VERB' and 'Tense=Past' in token.morph and 'VerbForm=Part' not in token.morph and re.match(r'^[A-Za-z]*$',token.text):\n",
    "            vlemma_cnt[token.lemma_] += 1\n",
    "            vlemma2past[token.lemma_].update([token.text.lower(),])\n",
    "            directObject = False\n",
    "            for item in token.children:\n",
    "                if item.pos_ not in ['NOUN','PRON']: continue\n",
    "                if (item.dep_ == \"dobj\"):\n",
    "                    directObject = True\n",
    "                if directObject == True:\n",
    "                    vlemma_transitive_cnt[token.lemma_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_to_use = []\n",
    "for vlemma, cnt in sorted(vlemma_cnt.items(), key=lambda x:-x[1]):\n",
    "    if cnt < MIN_FREQ: break\n",
    "    if vlemma in vlemma2past:\n",
    "        verb_past = vlemma2past[vlemma].most_common()[0][0]\n",
    "        s1 = vlemma_transitive_cnt[vlemma]/cnt\n",
    "        if s1>=.5 and vlemma not in mqnli_verbs and verb_past not in verbs_to_use:\n",
    "            verbs_to_use.append(verb_past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_transitive_verbs_past.txt', 'w') as f:\n",
    "    for item in verbs_to_use:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_to_use = []\n",
    "for lemma, cnt in sorted(nounlemma_cnt.items(), key=lambda x:-x[1]):\n",
    "    if cnt < MIN_FREQ: break\n",
    "    if lemma in nounlemma2pl:\n",
    "        noun_pl = nounlemma2pl[lemma].most_common()[0][0]\n",
    "        if noun_pl not in nouns_to_use and noun_pl not in verbs_to_use and lemma not in vlemma2past and lemma not in mqnli_nouns:\n",
    "            nouns_to_use.append(noun_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_plural_nouns.txt', 'w') as f:\n",
    "    for item in nouns_to_use:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463, 2185)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verbs_to_use),len(nouns_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and evaluate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'gpt2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "model.eval()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "\n",
    "def process(sentence):\n",
    "    tokens = [\"[CLS]\"] + tokenizer.tokenize(sentence)\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_ids = torch.tensor([tokens_ids,], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_ids, lm_labels=tokens_ids)\n",
    "        log_likelihood = outputs.item()\n",
    "    return np.exp(log_likelihood) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_NOUNS = 300\n",
    "USE_VERBS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for v_idx,verb in enumerate(verbs_to_use):\n",
    "    if v_idx>=USE_VERBS: break\n",
    "    for n_idx,subj in enumerate(nouns_to_use):\n",
    "        if n_idx>=USE_NOUNS: break\n",
    "        sent_body = 'two '+ subj + ' ' + nlp(verb)[0].lemma_ + ' somebody.'\n",
    "        ppl_body = process(sent_body)\n",
    "        sent_thing = 'two '+ subj + ' ' + nlp(verb)[0].lemma_ + ' something.'\n",
    "        ppl_thing = process(sent_thing)\n",
    "        sents.append((ppl_body,sent_body,n_idx,v_idx,'body'))\n",
    "        sents.append((ppl_thing,sent_thing,n_idx,v_idx,'thing'))\n",
    "sents = sorted(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_basic_sentences.tsv', 'w', encoding='utf-8') as ofh:\n",
    "    for s in sents:\n",
    "        print(\"\\t\".join(map(str, s)), file=ofh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SENTS=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_test_sentences.tsv', 'w', encoding='utf-8') as ofh:\n",
    "    for s in sents[:USE_SENTS]:\n",
    "        aff = 'the '+ nouns_to_use[s[2]] + ' ' + verbs_to_use[s[3]] + ' any'+s[4]+'.'\n",
    "        neg = 'the ' + nouns_to_use[s[2]] + ' did not ' + s[1].split(' ')[2]  + ' any'+s[4]+'.'\n",
    "        many = 'many '+ nouns_to_use[s[2]] + ' ' + verbs_to_use[s[3]] + ' any'+s[4]+'.'\n",
    "        few = 'few '+ nouns_to_use[s[2]] + ' ' + verbs_to_use[s[3]] + ' any'+s[4]+'.'\n",
    "        print(\"\\t\".join([aff,neg,many,few]+list(map(str, s[2:]))), file=ofh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
