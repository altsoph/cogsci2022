{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'tr' language\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import textacy.datasets\n",
    "import spacy_udpipe\n",
    "import re\n",
    "from anyascii import anyascii\n",
    "spacy_udpipe.download(\"tr\")\n",
    "nlp = spacy_udpipe.load(\"tr\")\n",
    "ds = textacy.datasets.Wikipedia(lang=\"tr\", version=\"current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlemma_cnt = defaultdict(int)\n",
    "nlemma2pl = defaultdict(Counter)\n",
    "nlemma2sg = defaultdict(Counter)\n",
    "vlemma_cnt = defaultdict(int)\n",
    "vlemma_transitive_cnt = defaultdict(int)\n",
    "vlemma2pastneg = defaultdict(Counter)\n",
    "vlemma2pastaff = defaultdict(Counter)\n",
    "\n",
    "for text,record in ds.records():\n",
    "    processed = nlp(text)\n",
    "    for token in processed:\n",
    "        if token.pos_ == 'NOUN' and 'Case=Nom' in token.morph and re.match(r'^[A-Za-z]*$',anyascii(token.text)):\n",
    "            nlemma_cnt[token.lemma_.lower()] += 1\n",
    "            if len([x for x in token.morph if re.search(r'psor',x)])==0:\n",
    "                if 'Number=Plur' in token.morph:\n",
    "                    nlemma2pl[token.lemma_.lower()].update([token.text.lower(),])\n",
    "                if 'Number=Sing' in token.morph:\n",
    "                    nlemma2sg[token.lemma_.lower()].update([token.text.lower(),])\n",
    "        if token.pos_ == 'VERB' and 'Number=Sing' in token.morph and 'Tense=Past' in token.morph and 'Person=3' in token.morph and 'Mood=Ind' in token.morph and 'Aspect=Perf' in token.morph and 'Voice=Cau' not in token.morph and 'Voice=Pass' not in token.morph and re.match(r'^[A-Za-z]*$',anyascii(token.text)):\n",
    "            vlemma_cnt[token.lemma_.lower()] += 1\n",
    "            if 'Polarity=Neg' in token.morph:\n",
    "                vlemma2pastneg[token.lemma_.lower()].update([token.text.lower(),])\n",
    "            if 'Polarity=Pos' in token.morph:\n",
    "                vlemma2pastaff[token.lemma_.lower()].update([token.text.lower(),])\n",
    "            directObject = False\n",
    "            for item in token.children:\n",
    "                if item.pos_ not in ['NOUN','PRON','PROPN']: continue\n",
    "                if item.dep_ == \"obj\" and 'Case=Acc' in item.morph:\n",
    "                    directObject = True\n",
    "            if directObject == True:\n",
    "                vlemma_transitive_cnt[token.lemma_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_to_use_aff = []\n",
    "verbs_to_use_neg = []\n",
    "for vlemma, cnt in sorted(vlemma_cnt.items(), key=lambda x:-x[1]):\n",
    "    if cnt < MIN_FREQ: break\n",
    "    if vlemma in vlemma2pastneg and vlemma in vlemma2pastaff:\n",
    "        verb_aff = vlemma2pastaff[vlemma].most_common()[0][0]\n",
    "        verb_neg = vlemma2pastneg[vlemma].most_common()[0][0]\n",
    "        s1 = vlemma_transitive_cnt[vlemma]/cnt\n",
    "        if s1>=.375 and verb_aff not in verbs_to_use_aff and verb_neg not in verbs_to_use_neg:\n",
    "            verbs_to_use_aff.append(verb_aff)\n",
    "            verbs_to_use_neg.append(verb_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 103)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verbs_to_use_aff),len(verbs_to_use_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transitive_verbs_aff.txt', 'w') as f:\n",
    "    for item in verbs_to_use_aff:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transitive_verbs_neg.txt', 'w') as f:\n",
    "    for item in verbs_to_use_neg:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_to_use_pl = []\n",
    "nouns_to_use_sg = []\n",
    "for lemma, cnt in sorted(nlemma_cnt.items(), key=lambda x:-x[1]):\n",
    "    if cnt < MIN_FREQ: break\n",
    "    if lemma in nlemma2pl and lemma in nlemma2sg:\n",
    "        noun_pl = nlemma2pl[lemma].most_common()[0][0]\n",
    "        noun_sg = nlemma2sg[lemma].most_common()[0][0]\n",
    "        if noun_pl not in nouns_to_use_pl+nouns_to_use_sg+verbs_to_use_aff+verbs_to_use_neg and noun_sg not in nouns_to_use_pl+nouns_to_use_sg+verbs_to_use_aff+verbs_to_use_neg: \n",
    "            nouns_to_use_pl.append(noun_pl)\n",
    "            nouns_to_use_sg.append(noun_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19378, 19378)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nouns_to_use_pl),len(nouns_to_use_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(nouns_to_use_pl,nouns_to_use_sg))[178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nouns_sg.txt', 'w') as f:\n",
    "    for item in nouns_to_use_sg:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nouns_pl.txt', 'w') as f:\n",
    "    for item in nouns_to_use_pl:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt_model_id = 'redrussianarmy/gpt2-turkish-cased'  \n",
    " \n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(gpt_model_id).to(device)\n",
    "gpt_model.eval()\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_model_id)\n",
    "\n",
    "def process(sentence):\n",
    "    encodings = gpt_tokenizer(sentence, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt_model(encodings.input_ids.to(device), labels=encodings.input_ids.to(device))\n",
    "        log_likelihood = outputs[0] * encodings.input_ids.size(1)\n",
    "    return float(log_likelihood.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.545171737670898"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process('çok güzel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_NOUNS = 350\n",
    "USE_VERBS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for v_idx,verb in enumerate(verbs_to_use_aff):\n",
    "    if v_idx>=USE_VERBS: break\n",
    "    for n_idx,subj in enumerate(nouns_to_use_sg):\n",
    "        if n_idx>=USE_NOUNS: break\n",
    "        sent_body = 'iki ' + subj + ' birini ' + verb + '.'\n",
    "        ppl_body = process(sent_body)\n",
    "        sent_thing = 'iki ' + subj + ' bir şey ' + verb + '.'\n",
    "        ppl_thing = process(sent_thing)\n",
    "        sents.append((ppl_body,sent_body,n_idx,v_idx,'body'))\n",
    "        sents.append((ppl_thing,sent_thing,n_idx,v_idx,'thing'))\n",
    "sents = sorted(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('basic_sentences.tsv', 'w', encoding='utf-8') as ofh:\n",
    "    for s in sents:\n",
    "        print(\"\\t\".join(map(str, s)), file=ofh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SENTS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_sentences.tsv', 'w', encoding='utf-8') as ofh:\n",
    "    for s in sents[:USE_SENTS]:\n",
    "        if s[-1]=='body':\n",
    "            aff = nouns_to_use_pl[int(s[2])] + ' kimseyi ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            neg = nouns_to_use_pl[int(s[2])] + ' kimseyi ' + verbs_to_use_neg[int(s[3])] + '.'\n",
    "            many = 'birçok '+ nouns_to_use_sg[int(s[2])] + ' kimseyi ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            few = 'birkaç '+ nouns_to_use_sg[int(s[2])] + ' kimseyi ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "        elif s[-1]=='thing':\n",
    "            aff = nouns_to_use_pl[int(s[2])] + ' hiçbir şey ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            neg = nouns_to_use_pl[int(s[2])] + ' hiçbir şey ' + verbs_to_use_neg[int(s[3])] + '.'\n",
    "            many = 'birçok '+ nouns_to_use_sg[int(s[2])] + ' hiçbir şey ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            few = 'birkaç '+ nouns_to_use_sg[int(s[2])] + ' hiçbir şey ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "        print(\"\\t\".join([aff,neg,many,few]+list(map(str, s[2:]))), file=ofh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_sentences_cok_az.tsv', 'w', encoding='utf-8') as ofh:\n",
    "    for s in sents[:USE_SENTS]:\n",
    "        if s[-1]=='body':\n",
    "            aff = nouns_to_use_pl[int(s[2])] + ' kimseyi ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            neg = nouns_to_use_pl[int(s[2])] + ' kimseyi ' + verbs_to_use_neg[int(s[3])] + '.'\n",
    "            many = 'birçok '+ nouns_to_use_sg[int(s[2])] + ' kimseyi ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            few = 'çok az '+ nouns_to_use_sg[int(s[2])] + ' kimseyi ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "        elif s[-1]=='thing':\n",
    "            aff = nouns_to_use_pl[int(s[2])] + ' hiçbir şey ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            neg = nouns_to_use_pl[int(s[2])] + ' hiçbir şey ' + verbs_to_use_neg[int(s[3])] + '.'\n",
    "            many = 'birçok '+ nouns_to_use_sg[int(s[2])] + ' hiçbir şey ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "            few = 'çok az '+ nouns_to_use_sg[int(s[2])] + ' hiçbir şey ' + verbs_to_use_aff[int(s[3])] + '.'\n",
    "        print(\"\\t\".join([aff,neg,many,few]+list(map(str, s[2:]))), file=ofh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
