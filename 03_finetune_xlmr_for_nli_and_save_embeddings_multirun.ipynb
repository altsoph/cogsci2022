{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VlYo1CIaQLY7"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(13370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",sentence1,sentence2,gold_label,sent1_readable,sent2_readable,sent1_subj_quant,sent1_subj_adj,sent1_obj_quant,sent1_obj_adj,sent1_adv,sent1_polarity,sent2_subj_quant,sent2_subj_adj,sent2_obj_quant,sent2_obj_adj,sent2_adv,sent2_polarity\r\n",
      "8,some slow gymnast emptystring emptystring publicizes notevery emptystring straw,some emptystring gymnast emptystring jealously publicizes every opaque straw,neutral,some slow gymnast publicizes not every straw,some gymnast jealously publicizes every opaque straw,some,yes,notevery,no,no,aff,some,no,every,yes,yes,aff\r\n",
      "12,no Spanish receiver emptystring emptystring publicizes some dry lemur,every emptystring receiver emptystring fortunately publicizes every dry lemur,contradiction,no spanish receiver publicizes some dry lemur,every receiver fortunately publicizes every dry lemur,no,yes,some,yes,no,aff,every,no,every,yes,yes,aff\r\n",
      "13,no furry linguist emptystring deftly draws no pink lemur,every furry linguist emptystring deftly draws every emptystring lemur,neutral,no furry linguist deftly draws no pink lemur,every furry linguist deftly draws every lemur,no,yes,no,yes,yes,aff,every,yes,every,no,yes,aff\r\n",
      "24,every terrible chef emptystring poorly claims every emptystring crown,no terrible chef emptystring emptystring claims every soft crown,contradiction,every terrible chef poorly claims every crown,no terrible chef claims every soft crown,every,yes,every,no,yes,aff,no,yes,every,yes,no,aff\r\n",
      "27,no friendly princess emptystring absentmindedly receives notevery Mexican keyboard,every emptystring princess emptystring absentmindedly receives no Mexican dog,neutral,no friendly princess absentmindedly receives not every mexican keyboard,every princess absentmindedly receives no mexican dog,no,yes,notevery,yes,yes,aff,every,no,no,yes,yes,aff\r\n",
      "34,no emptystring milkman emptystring wearily faces no smelly harp,some Californian milkman emptystring emptystring faces no emptystring harp,contradiction,no milkman wearily faces no smelly harp,some californian milkman faces no harp,no,no,no,yes,yes,aff,some,yes,no,no,no,aff\r\n",
      "37,notevery slimy clown emptystring wildly washes some emptystring mug,notevery emptystring clown emptystring emptystring washes every common mug,neutral,not every slimy clown wildly washes some mug,not every clown washes every common mug,notevery,yes,some,no,yes,aff,notevery,no,every,yes,no,aff\r\n",
      "39,notevery Chinese playwright emptystring deftly slaps every cyan bag,no Chinese playwright emptystring deftly slaps notevery cyan bag,contradiction,not every chinese playwright deftly slaps every cyan bag,no chinese playwright deftly slaps not every cyan bag,notevery,yes,every,yes,yes,aff,no,yes,notevery,yes,yes,aff\r\n",
      "43,notevery emptystring traitor emptystring mysteriously receives notevery Japanese pillow,no French traitor emptystring roughly reveres some emptystring pillow,neutral,not every traitor mysteriously receives not every japanese pillow,no french traitor roughly reveres some pillow,notevery,no,notevery,yes,yes,aff,no,yes,some,no,yes,aff\r\n"
     ]
    }
   ],
   "source": [
    "!head stage1_training_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 sent1_subj_adj\n",
    "# 9 sent1_obj_adj\n",
    "# 10 sent1_adv,sent1_polarity,sent2_subj_quant,sent2_subj_adj,sent2_obj_quant,sent2_obj_adj,sent2_adv,sent2_polarity\n",
    "\n",
    "sadj1 = set()\n",
    "subj1 = set()\n",
    "adv1 = set()\n",
    "verb1 = set()\n",
    "oadj1 = set()\n",
    "obj1 = set()\n",
    "\n",
    "sadj2 = set()\n",
    "subj2 = set()\n",
    "adv2 = set()\n",
    "verb2 = set()\n",
    "oadj2 = set()\n",
    "obj2 = set()\n",
    "\n",
    "for idx, line in enumerate(open('stage1_training_data.csv')):\n",
    "    if idx:\n",
    "        chunks = line.strip().split(',')\n",
    "        assert len(chunks)==18, 'parse error'\n",
    "        words = chunks[1].split()\n",
    "        sadj1.add(words[1])\n",
    "        subj1.add(words[2])\n",
    "        adv1.add(words[4])\n",
    "        verb1.add(words[5])\n",
    "        oadj1.add(words[7])\n",
    "        obj1.add(words[8])\n",
    "        words = chunks[2].split()\n",
    "        sadj2.add(words[1])\n",
    "        subj2.add(words[2])\n",
    "        adv2.add(words[4])\n",
    "        verb2.add(words[5])\n",
    "        oadj2.add(words[7])\n",
    "        obj2.add(words[8])\n",
    "\n",
    "set_random_seed(0xABBA+1)\n",
    "\n",
    "sadjs = list(sadj1-set(('emptystring',)))\n",
    "np.random.shuffle(sadjs)\n",
    "\n",
    "subjs = list(subj1-set(('emptystring',)))\n",
    "np.random.shuffle(subjs)\n",
    "\n",
    "objs = list(obj1-set(('emptystring',)))\n",
    "np.random.shuffle(objs)\n",
    "\n",
    "advs = list(adv1-set(('emptystring',)))\n",
    "np.random.shuffle(advs)\n",
    "\n",
    "oadjs = list(oadj1-set(('emptystring',)))\n",
    "np.random.shuffle(oadjs)\n",
    "\n",
    "verbs = list(verb1-set(('emptystring',)))\n",
    "np.random.shuffle(verbs)\n",
    "\n",
    "TRAIN_PART = 60\n",
    "VALID_PART = 1000\n",
    "\n",
    "train_sadjs = sadjs[:TRAIN_PART]+['emptystring',]\n",
    "train_subjs = subjs[:TRAIN_PART]+['emptystring',]\n",
    "train_objs = objs[:TRAIN_PART]+['emptystring',]\n",
    "train_advs = advs[:TRAIN_PART]+['emptystring',]\n",
    "train_oadjs = oadjs[:TRAIN_PART]+['emptystring',]\n",
    "train_verbs = verbs[:TRAIN_PART]+['emptystring',]\n",
    "\n",
    "valid_sadjs = sadjs[TRAIN_PART:VALID_PART]+['emptystring',]\n",
    "valid_subjs = subjs[TRAIN_PART:VALID_PART]+['emptystring',]\n",
    "valid_objs =  objs[TRAIN_PART:VALID_PART]+['emptystring',]\n",
    "valid_advs =  advs[TRAIN_PART:VALID_PART]+['emptystring',]\n",
    "valid_oadjs = oadjs[TRAIN_PART:VALID_PART]+['emptystring',]\n",
    "valid_verbs = verbs[TRAIN_PART:VALID_PART]+['emptystring',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ukranian', 'helpful', 'proud', 'slimy', 'Mississippian', 'German', 'Spanish', 'stupid', 'thoughtful', 'noble', 'outstanding', 'lucky', 'polite', 'Indian', 'fuzzy', 'crazy', 'devout', 'hopeful', 'Alaskan', 'English', 'Panamanian', 'Italian', 'religious', 'small', 'beautiful', 'angry', 'rude', 'Indonesian', 'insane', 'scatterbrained', 'unhinged', 'Texan', 'Korean', 'sly', 'friendly', 'idealistic', 'uncouth', 'rough', 'loving', 'Polish', 'untrustworthy', 'Austrian', 'furry', 'charismatic', 'Mongolian', 'Latvian', 'quick', 'unpredictable', 'Moroccan', 'Californian', 'kooky', 'Mexican', 'Alabaman', 'smooth', 'Afghani', 'Chinese', 'unhelpful', 'surly', 'underwhelming', 'Washingtonian', 'emptystring']\n",
      "['Oregonian', 'soft', 'French', 'chummy', 'silly', 'Oklahoman', 'boisterous', 'Canadian', 'horrifying', 'coy', 'sheltered', 'taciturn', 'seductive', 'Japanese', 'vivacious', 'Israeli', 'Nebraskan', 'Siberian', 'happy', 'sneaky', 'Hawaiian', 'big', 'slow', 'sad', 'savvy', 'Lithuanian', 'Pakistani', 'loud', 'Swiss', 'Columbian', 'wild', 'scaly', 'burly', 'antagonistic', 'talkative', 'jealous', 'ugly', 'terrible', 'irresistible', 'preposterous', 'emptystring']\n"
     ]
    }
   ],
   "source": [
    "print(train_sadjs)\n",
    "print(valid_sadjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475998\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "for idx, line in enumerate(open('stage1_training_data.csv')):\n",
    "    if idx:\n",
    "        chunks = line.strip().split(',')\n",
    "        assert len(chunks)==18, 'parse error'\n",
    "        records.append( {'left':chunks[4], 'right': chunks[5], 'label':chunks[3]})\n",
    "\n",
    "np.random.shuffle(records)\n",
    "print(len(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20117\n"
     ]
    }
   ],
   "source": [
    "train_records = []\n",
    "\n",
    "for idx, line in enumerate(open('stage1_training_data.csv')):\n",
    "    if idx:\n",
    "        chunks = line.strip().split(',')\n",
    "        assert len(chunks)==18, 'parse error'\n",
    "        words = chunks[1].split()\n",
    "        if  words[1] not in train_sadjs or \\\n",
    "            words[2] not in train_subjs or \\\n",
    "            words[4] not in train_advs or \\\n",
    "            words[5] not in train_verbs or \\\n",
    "            words[7] not in train_oadjs or \\\n",
    "            words[8] not in train_objs: continue\n",
    "        words = chunks[2].split()\n",
    "        if  words[1] not in train_sadjs or \\\n",
    "            words[2] not in train_subjs or \\\n",
    "            words[4] not in train_advs or \\\n",
    "            words[5] not in train_verbs or \\\n",
    "            words[7] not in train_oadjs or \\\n",
    "            words[8] not in train_objs: continue\n",
    "        train_records.append( {'left':chunks[4], 'right': chunks[5], 'label':chunks[3]})\n",
    "\n",
    "np.random.shuffle(train_records)\n",
    "print(len(train_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622\n"
     ]
    }
   ],
   "source": [
    "val_records = []\n",
    "\n",
    "for idx, line in enumerate(open('stage1_training_data.csv')):\n",
    "    if idx:\n",
    "        chunks = line.strip().split(',')\n",
    "        assert len(chunks)==18, 'parse error'\n",
    "        words = chunks[1].split()\n",
    "        if  words[1] not in valid_sadjs or \\\n",
    "            words[2] not in valid_subjs or \\\n",
    "            words[8] not in valid_objs: continue\n",
    "        words = chunks[2].split()\n",
    "        if  words[1] not in valid_sadjs or \\\n",
    "            words[2] not in valid_subjs or \\\n",
    "            words[4] not in valid_advs or \\\n",
    "            words[5] not in valid_verbs or \\\n",
    "            words[7] not in valid_oadjs or \\\n",
    "            words[8] not in valid_objs: continue\n",
    "        val_records.append( {'left':chunks[4], 'right': chunks[5], 'label':chunks[3]})\n",
    "\n",
    "np.random.shuffle(val_records)\n",
    "print(len(val_records))\n",
    "# val_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1811 1811\n"
     ]
    }
   ],
   "source": [
    "test_records = val_records[:len(val_records)//2]\n",
    "val_records = val_records[-len(val_records)//2:]\n",
    "print(len(test_records), len(val_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAL_SHARE = .1\n",
    "# TEST_SHARE = .1\n",
    "\n",
    "# val_records = records[:int(len(records)*VAL_SHARE)]\n",
    "# test_records = records[-int(len(records)*TEST_SHARE):]\n",
    "# train_records = records[int(len(records)*VAL_SHARE):-int(len(records)*TEST_SHARE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN65SEFpOOmI",
    "outputId": "858b9f6f-a215-4175-8ef1-0d259ebeb1f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'contradiction': 159754, 'neutral': 158900, 'entailment': 157344}) 475998\n",
      "['entailment', 'contradiction', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels_counter = Counter([r[\"label\"] for r in records])\n",
    "print(labels_counter, sum(labels_counter.values()))\n",
    "labels = list(labels_counter.keys())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class NLIsDataset(Dataset):\n",
    "    def __init__(self, records, tokenizer, max_tokens, labels):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.records = records\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    \n",
    "    def embed_record(self, record):\n",
    "        inputs = self.tokenizer(\n",
    "            text=record[\"left\"],\n",
    "            text_pair=record[\"right\"],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_tokens,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"longest_first\",\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        for key, value in inputs.items():\n",
    "            value.squeeze_(0)\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        record = self.records[index]\n",
    "        output = self.embed_record(record)\n",
    "        label = record.get(\"label\", None)\n",
    "        if label is not None and label in self.labels:\n",
    "            output[\"labels\"] = torch.tensor(self.labels.index(label))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xlm-roberta-large\"\n",
    "# MODEL_NAME = 'bert-base-multilingual-uncased'\n",
    "TOKENIZER_NAME = MODEL_NAME\n",
    "MAX_TOKENS = 100\n",
    "EPOCHS = 4*32 # 4\n",
    "EVAL_STEPS = 32*4 # 32\n",
    "WARMUP_STEPS = 16\n",
    "LR = 0.00002\n",
    "BATCH_SIZE = 32*4 # 32\n",
    "GRAD_ACCUM_STEPS = 1 # 4\n",
    "PATIENCE = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GRAD_ACCUM_STEPS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, do_lower_case=False)\n",
    "train_data = NLIsDataset(train_records, tokenizer, MAX_TOKENS, labels)\n",
    "val_data = NLIsDataset(val_records, tokenizer, MAX_TOKENS, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wIxaG3SBS9z-",
    "outputId": "7144b605-71e0-4b65-8b74-c2a575a21622",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 20:30:01.627929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 19845\n",
      "  Num Epochs = 128\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 19840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='512' max='19840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  512/19840 25:55 < 16:22:12, 0.33 it/s, Epoch 3/128]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>0.788705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.677800</td>\n",
       "      <td>0.534601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.537800</td>\n",
       "      <td>0.588231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.543700</td>\n",
       "      <td>0.597887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1793\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-128\n",
      "Configuration saved in checkpoints/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1793\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-256\n",
      "Configuration saved in checkpoints/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1793\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-384\n",
      "Configuration saved in checkpoints/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints/checkpoint-384/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1793\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-512\n",
      "Configuration saved in checkpoints/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-384] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints/checkpoint-256 (score: 0.5346008539199829).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=512, training_loss=0.6989619880914688, metrics={'train_runtime': 1557.4109, 'train_samples_per_second': 1631.015, 'train_steps_per_second': 12.739, 'total_flos': 2.20209475664142e+16, 'train_loss': 0.6989619880914688, 'epoch': 3.3})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_steps=EVAL_STEPS,\n",
    "    save_steps=EVAL_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "!rm -rf checkpoints\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_batch(data, batch_size):\n",
    "    start_index = 0\n",
    "    while start_index < len(data):\n",
    "        end_index = start_index + batch_size\n",
    "        batch = data[start_index:end_index]\n",
    "        yield batch\n",
    "        start_index = end_index\n",
    "\n",
    "def pipe_predict(data, pipe, batch_size=64):\n",
    "    raw_preds = []\n",
    "    for batch in tqdm(get_batch(data, batch_size)):\n",
    "        raw_preds += pipe(batch)\n",
    "    preds = np.array([int(max(labels, key=lambda x: x[\"score\"])[\"label\"][-1]) for labels in raw_preds])\n",
    "    pp = np.array([[l[\"score\"] for l in labels] for labels in raw_preds])\n",
    "    return preds, pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/altsoph/Volume/_py37/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c10d4d1986f48c4a0841dd3a99e702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.630     0.703     0.665       667\n",
      "           1      0.630     0.635     0.632       654\n",
      "           2      0.939     0.782     0.853       490\n",
      "\n",
      "    accuracy                          0.700      1811\n",
      "   macro avg      0.733     0.706     0.717      1811\n",
      "weighted avg      0.714     0.700     0.704      1811\n",
      "\n",
      "[[469 198   0]\n",
      " [214 415  25]\n",
      " [ 61  46 383]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, framework=\"pt\", device=0, return_all_scores=True)\n",
    "\n",
    "y_true = np.array([labels.index(r[\"label\"]) for r in test_records], dtype=np.int32)\n",
    "test_pairs = [(r[\"left\"], r[\"right\"]) for r in test_records]\n",
    "\n",
    "y_pred, y_pred_prob = pipe_predict(test_pairs, pipe)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=3))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 22:41:31.197354: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "### load model\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, EarlyStoppingCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "records2 = []\n",
    "\n",
    "for idx, line in enumerate(open('stage2_training_data.csv')):\n",
    "    if idx:\n",
    "        chunks = line.strip().split(',')\n",
    "        assert len(chunks)==18, 'parse error'\n",
    "        records2.append( {'left':chunks[4], 'right': chunks[5], 'label':chunks[3]})\n",
    "\n",
    "np.random.shuffle(records2)\n",
    "# records2[:5]\n",
    "VAL_SHARE = .1\n",
    "TEST_SHARE = .1\n",
    "\n",
    "val_records2 = records2[:int(len(records2)*VAL_SHARE)]\n",
    "test_records2 = records2[-int(len(records2)*TEST_SHARE):]\n",
    "train_records2 = records2[int(len(records2)*VAL_SHARE):-int(len(records2)*TEST_SHARE)]\n",
    "\n",
    "# tokenizer.convert_ids_to_tokens(val_data2[0]['input_ids'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 100\n",
    "EPOCHS = 4 # 4\n",
    "EVAL_STEPS = 32 # 32\n",
    "WARMUP_STEPS = 16\n",
    "LR = 0.00002\n",
    "BATCH_SIZE = 32 # 32\n",
    "GRAD_ACCUM_STEPS = 4 # 4\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='224' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [224/600 18:45 < 31:46, 0.20 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.611844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.504875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.332611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.286006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.298301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.288747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.309400</td>\n",
       "      <td>0.296096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-128 (score: 0.2860058546066284).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8555be8a1f684ed89c56b658a6347cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.824     0.949     0.882       930\n",
      "           1      0.839     0.838     0.839       710\n",
      "           2      0.998     0.813     0.896       760\n",
      "\n",
      "    accuracy                          0.873      2400\n",
      "   macro avg      0.887     0.867     0.872      2400\n",
      "weighted avg      0.884     0.873     0.874      2400\n",
      "\n",
      "[[883  47   0]\n",
      " [114 595   1]\n",
      " [ 75  67 618]]\n",
      "=== 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/600 20:57 < 22:51, 0.23 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.649300</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.493425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.363782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.335800</td>\n",
       "      <td>0.317266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.307298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>0.281547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.312211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.283001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.283099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-192 (score: 0.28154686093330383).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcfb583b0224a80b281505d0d215591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.873     0.896     0.884       930\n",
      "           1      0.795     0.921     0.853       710\n",
      "           2      1.000     0.820     0.901       760\n",
      "\n",
      "    accuracy                          0.879      2400\n",
      "   macro avg      0.889     0.879     0.879      2400\n",
      "weighted avg      0.890     0.879     0.880      2400\n",
      "\n",
      "[[833  97   0]\n",
      " [ 56 654   0]\n",
      " [ 65  72 623]]\n",
      "=== 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 43:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.561477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.389002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.384404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.359900</td>\n",
       "      <td>0.352104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>0.309867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.304017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.300108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.302374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.285573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.272081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.265130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.264992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>0.262938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.272430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.259422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>0.266302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.251900</td>\n",
       "      <td>0.254814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>0.259771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-544 (score: 0.2548137903213501).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0d4dbca1e74652936c9a28008774e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.881     0.900     0.890       930\n",
      "           1      0.833     0.921     0.875       710\n",
      "           2      0.998     0.874     0.932       760\n",
      "\n",
      "    accuracy                          0.898      2400\n",
      "   macro avg      0.904     0.898     0.899      2400\n",
      "weighted avg      0.904     0.898     0.899      2400\n",
      "\n",
      "[[837  92   1]\n",
      " [ 56 654   0]\n",
      " [ 57  39 664]]\n",
      "=== 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/600 20:29 < 27:44, 0.21 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.649400</td>\n",
       "      <td>0.553621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.446725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.427800</td>\n",
       "      <td>0.337199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.319624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.287654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.301104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.289547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>0.301638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-160 (score: 0.2876538038253784).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a12993fb0b4011b9417ddbe7a57d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.839     0.911     0.874       930\n",
      "           1      0.804     0.870     0.836       710\n",
      "           2      1.000     0.818     0.900       760\n",
      "\n",
      "    accuracy                          0.870      2400\n",
      "   macro avg      0.881     0.867     0.870      2400\n",
      "weighted avg      0.880     0.870     0.871      2400\n",
      "\n",
      "[[847  83   0]\n",
      " [ 92 618   0]\n",
      " [ 70  68 622]]\n",
      "=== 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 40:56, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.645300</td>\n",
       "      <td>0.581974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.384228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.395785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.322302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.307622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.356715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.313800</td>\n",
       "      <td>0.305237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.320274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>0.304633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.305694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.279798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.287515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.268596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.273407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.265640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.245500</td>\n",
       "      <td>0.261638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.265341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.259136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.2591356039047241).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40636120150495abaa9293174a33cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.910     0.845     0.876       930\n",
      "           1      0.787     0.976     0.872       710\n",
      "           2      0.998     0.862     0.925       760\n",
      "\n",
      "    accuracy                          0.889      2400\n",
      "   macro avg      0.899     0.894     0.891      2400\n",
      "weighted avg      0.902     0.889     0.890      2400\n",
      "\n",
      "[[786 144   0]\n",
      " [ 16 693   1]\n",
      " [ 62  43 655]]\n",
      "=== 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 43:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>0.544878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.469915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.396700</td>\n",
       "      <td>0.327815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.305004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.304643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.311300</td>\n",
       "      <td>0.309040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>0.290654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.307215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.284343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.339350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.304462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.274610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.288659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.293800</td>\n",
       "      <td>0.273464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.279411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.268545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.272714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.273077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-512 (score: 0.2685452103614807).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c940359ddc5440182b39e5901d978a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.901     0.870     0.885       930\n",
      "           1      0.791     0.963     0.869       710\n",
      "           2      1.000     0.838     0.912       760\n",
      "\n",
      "    accuracy                          0.887      2400\n",
      "   macro avg      0.897     0.890     0.889      2400\n",
      "weighted avg      0.900     0.887     0.889      2400\n",
      "\n",
      "[[809 121   0]\n",
      " [ 26 684   0]\n",
      " [ 63  60 637]]\n",
      "=== 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/600 26:49 < 15:09, 0.24 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.552695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.436694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.400200</td>\n",
       "      <td>0.306241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.330227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.291994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.287323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>0.288442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.367933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.278096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.295600</td>\n",
       "      <td>0.289362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.281040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.292400</td>\n",
       "      <td>0.281309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-288 (score: 0.27809596061706543).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d336d0cc3d4543e6a6effa205c90c9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.810     0.989     0.891       930\n",
      "           1      0.889     0.804     0.845       710\n",
      "           2      1.000     0.818     0.900       760\n",
      "\n",
      "    accuracy                          0.880      2400\n",
      "   macro avg      0.900     0.871     0.878      2400\n",
      "weighted avg      0.894     0.880     0.880      2400\n",
      "\n",
      "[[920  10   0]\n",
      " [139 571   0]\n",
      " [ 77  61 622]]\n",
      "=== 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/600 20:59 < 22:53, 0.23 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>0.574417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.599700</td>\n",
       "      <td>0.474865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.365645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.295695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.294375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.289612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.309800</td>\n",
       "      <td>0.293854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.307688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.294859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-192 (score: 0.28961190581321716).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df798526c1d74551ad1b3a4af608dde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.902     0.855     0.878       930\n",
      "           1      0.770     0.969     0.858       710\n",
      "           2      0.997     0.821     0.900       760\n",
      "\n",
      "    accuracy                          0.878      2400\n",
      "   macro avg      0.890     0.882     0.879      2400\n",
      "weighted avg      0.893     0.878     0.879      2400\n",
      "\n",
      "[[795 133   2]\n",
      " [ 22 688   0]\n",
      " [ 64  72 624]]\n",
      "=== 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/600 41:45 < 01:44, 0.23 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.567145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.538261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.392141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.313771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.304046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.292961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.288316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.279418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.283258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.288058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.271630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.267047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.267820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.265975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.261974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.266083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.274673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.239800</td>\n",
       "      <td>0.267566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-480 (score: 0.2619735598564148).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c50e32dbad4567948e896ffec5d40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.808     0.996     0.892       930\n",
      "           1      0.947     0.799     0.866       710\n",
      "           2      1.000     0.862     0.926       760\n",
      "\n",
      "    accuracy                          0.895      2400\n",
      "   macro avg      0.918     0.885     0.895      2400\n",
      "weighted avg      0.910     0.895     0.895      2400\n",
      "\n",
      "[[926   4   0]\n",
      " [143 567   0]\n",
      " [ 77  28 655]]\n",
      "=== 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 40:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.573478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>0.444769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.347294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.343500</td>\n",
       "      <td>0.334711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.291434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.285594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.280634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.298517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.291400</td>\n",
       "      <td>0.275886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.286649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.286501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.272784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.268650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.270750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>0.262663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.265908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.254568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.2545681893825531).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731d4b35222d49a3a92d0a3017bdf8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.878     0.909     0.893       930\n",
      "           1      0.837     0.908     0.871       710\n",
      "           2      1.000     0.878     0.935       760\n",
      "\n",
      "    accuracy                          0.899      2400\n",
      "   macro avg      0.905     0.898     0.900      2400\n",
      "weighted avg      0.905     0.899     0.900      2400\n",
      "\n",
      "[[845  85   0]\n",
      " [ 65 645   0]\n",
      " [ 52  41 667]]\n",
      "=== 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [544/600 38:31 < 03:58, 0.23 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.561063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.544015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.334429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.294773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.292613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.285395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.278058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.285480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.275199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.275276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.282093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.274946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.267509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.258516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.269412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.271659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.260489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-448 (score: 0.2585156559944153).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c088948d0b49ad880b1e371f8c3e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.810     0.984     0.888       930\n",
      "           1      0.931     0.803     0.862       710\n",
      "           2      1.000     0.866     0.928       760\n",
      "\n",
      "    accuracy                          0.893      2400\n",
      "   macro avg      0.914     0.884     0.893      2400\n",
      "weighted avg      0.906     0.893     0.893      2400\n",
      "\n",
      "[[915  15   0]\n",
      " [140 570   0]\n",
      " [ 75  27 658]]\n",
      "=== 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 41:42, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>0.560790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.562700</td>\n",
       "      <td>0.391489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.323639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.324580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.294962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.288300</td>\n",
       "      <td>0.282401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.285197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>0.282153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>0.284206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.273352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.277158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.274193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.271314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.270295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.275701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.270162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.264364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.258700</td>\n",
       "      <td>0.263493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.2634933590888977).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724d568862d54c919c1d6055414f95d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.902     0.867     0.884       930\n",
      "           1      0.794     0.966     0.872       710\n",
      "           2      0.998     0.843     0.914       760\n",
      "\n",
      "    accuracy                          0.889      2400\n",
      "   macro avg      0.898     0.892     0.890      2400\n",
      "weighted avg      0.900     0.889     0.890      2400\n",
      "\n",
      "[[806 123   1]\n",
      " [ 24 686   0]\n",
      " [ 64  55 641]]\n",
      "=== 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/600 22:52 < 20:08, 0.23 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.658800</td>\n",
       "      <td>0.585656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.395839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.314577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.298923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.307800</td>\n",
       "      <td>0.289032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.294787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.283501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.310700</td>\n",
       "      <td>0.312226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.294133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>0.307167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-224 (score: 0.283500611782074).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabf0e72ca154bbf845221da7cf8df29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.821     0.965     0.887       930\n",
      "           1      0.857     0.830     0.843       710\n",
      "           2      0.997     0.814     0.896       760\n",
      "\n",
      "    accuracy                          0.877      2400\n",
      "   macro avg      0.892     0.870     0.876      2400\n",
      "weighted avg      0.888     0.877     0.877      2400\n",
      "\n",
      "[[897  31   2]\n",
      " [121 589   0]\n",
      " [ 74  67 619]]\n",
      "=== 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='224' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [224/600 15:25 < 26:07, 0.24 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>0.543392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>0.468254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.311522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.306015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.307631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.310182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.319086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-128 (score: 0.30601540207862854).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b028cd0a8d4e455d9ca866e8461a2812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.870     0.889     0.879       930\n",
      "           1      0.781     0.917     0.843       710\n",
      "           2      0.995     0.805     0.890       760\n",
      "\n",
      "    accuracy                          0.871      2400\n",
      "   macro avg      0.882     0.870     0.871      2400\n",
      "weighted avg      0.883     0.871     0.872      2400\n",
      "\n",
      "[[827 102   1]\n",
      " [ 57 651   2]\n",
      " [ 67  81 612]]\n",
      "=== 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 43:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.665800</td>\n",
       "      <td>0.570557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.429299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.397200</td>\n",
       "      <td>0.303637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.309021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.289322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.296641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>0.287037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.277308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.275571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>0.268541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.268804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.279557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.267927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.259105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.266739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.256824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.255221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.25522109866142273).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd7682254d544cf9f0aff8bc3680b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.855     0.915     0.884       930\n",
      "           1      0.844     0.894     0.869       710\n",
      "           2      1.000     0.859     0.924       760\n",
      "\n",
      "    accuracy                          0.891      2400\n",
      "   macro avg      0.900     0.890     0.892      2400\n",
      "weighted avg      0.898     0.891     0.892      2400\n",
      "\n",
      "[[851  79   0]\n",
      " [ 75 635   0]\n",
      " [ 69  38 653]]\n",
      "=== 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 40:02, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>0.574034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>0.378323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.326337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.301332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.288852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.307343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>0.305105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>0.282263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.329936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.285974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.278704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.285306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.295600</td>\n",
       "      <td>0.278588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.272820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.273229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.269337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>0.267081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.2670813500881195).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f891a0a7b294c67ac5171d2667691cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.898     0.858     0.877       930\n",
      "           1      0.780     0.955     0.859       710\n",
      "           2      0.992     0.838     0.909       760\n",
      "\n",
      "    accuracy                          0.880      2400\n",
      "   macro avg      0.890     0.884     0.882      2400\n",
      "weighted avg      0.893     0.880     0.882      2400\n",
      "\n",
      "[[798 132   0]\n",
      " [ 27 678   5]\n",
      " [ 64  59 637]]\n",
      "=== 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='352' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [352/600 24:34 < 17:24, 0.24 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.568061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.541300</td>\n",
       "      <td>0.447667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.305667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.308170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.332137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.295904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.294811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.276301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.299800</td>\n",
       "      <td>0.278149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.289672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.285971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-256 (score: 0.2763010263442993).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf6d04920ce4e58a47e5660a2da86dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.802     0.995     0.888       930\n",
      "           1      0.906     0.785     0.841       710\n",
      "           2      0.992     0.824     0.900       760\n",
      "\n",
      "    accuracy                          0.878      2400\n",
      "   macro avg      0.900     0.868     0.876      2400\n",
      "weighted avg      0.893     0.878     0.878      2400\n",
      "\n",
      "[[925   3   2]\n",
      " [150 557   3]\n",
      " [ 79  55 626]]\n",
      "=== 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/600 34:52 < 08:45, 0.23 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.670800</td>\n",
       "      <td>0.557689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.382609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.326840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.336934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.300940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.279699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.278612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>0.289431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.292265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.274527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.294548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.269581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.276976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.272156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.275625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-384 (score: 0.2695808410644531).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bea1a7bd08446c946d7fba5bb8ac3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.803     0.996     0.889       930\n",
      "           1      0.917     0.794     0.851       710\n",
      "           2      1.000     0.832     0.908       760\n",
      "\n",
      "    accuracy                          0.884      2400\n",
      "   macro avg      0.907     0.874     0.883      2400\n",
      "weighted avg      0.899     0.884     0.884      2400\n",
      "\n",
      "[[926   4   0]\n",
      " [146 564   0]\n",
      " [ 81  47 632]]\n",
      "=== 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 40:57, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.672900</td>\n",
       "      <td>0.566564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.601200</td>\n",
       "      <td>0.499637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.307910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.299934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.290145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>0.298411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.292125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.283419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>0.294209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.299600</td>\n",
       "      <td>0.289736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.279532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.276390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.265009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.265896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>0.260217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.262912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.245500</td>\n",
       "      <td>0.253709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>0.254760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-544 (score: 0.25370919704437256).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4477b1940ae24398bc52d3012d388e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.916     0.844     0.879       930\n",
      "           1      0.788     0.985     0.875       710\n",
      "           2      1.000     0.863     0.927       760\n",
      "\n",
      "    accuracy                          0.892      2400\n",
      "   macro avg      0.901     0.897     0.894      2400\n",
      "weighted avg      0.905     0.892     0.893      2400\n",
      "\n",
      "[[785 145   0]\n",
      " [ 11 699   0]\n",
      " [ 61  43 656]]\n",
      "=== 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/600 20:07 < 27:15, 0.21 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.677800</td>\n",
       "      <td>0.612598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.604100</td>\n",
       "      <td>0.513536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.308489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.291397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.283657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>0.301376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.289340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.285360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-160 (score: 0.28365686535835266).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357f638dd5c143b4a0780baa257e5304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.863     0.930     0.895       930\n",
      "           1      0.823     0.901     0.860       710\n",
      "           2      1.000     0.816     0.899       760\n",
      "\n",
      "    accuracy                          0.885      2400\n",
      "   macro avg      0.895     0.882     0.885      2400\n",
      "weighted avg      0.895     0.885     0.886      2400\n",
      "\n",
      "[[865  65   0]\n",
      " [ 70 640   0]\n",
      " [ 67  73 620]]\n",
      "=== 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 40:20, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>0.559476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.354897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.303312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.288694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.299732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.286945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.282437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.277132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.277664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.274398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.267221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.261796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.271258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.271442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.260298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.260222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.256316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.2563156187534332).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dd6d0c257a4f259cc40c7201df06c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.832     0.871       930\n",
      "           1      0.778     0.977     0.866       710\n",
      "           2      1.000     0.870     0.930       760\n",
      "\n",
      "    accuracy                          0.887      2400\n",
      "   macro avg      0.897     0.893     0.889      2400\n",
      "weighted avg      0.901     0.887     0.888      2400\n",
      "\n",
      "[[774 156   0]\n",
      " [ 16 694   0]\n",
      " [ 57  42 661]]\n",
      "=== 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/600 22:04 < 24:04, 0.22 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.712323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.563900</td>\n",
       "      <td>0.492532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.315638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.314602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>0.295029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.300200</td>\n",
       "      <td>0.278170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.294191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>0.293926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>0.290067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-192 (score: 0.2781704068183899).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac5686e884149b3a3800c4d9de709a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.886     0.862     0.874       930\n",
      "           1      0.770     0.945     0.849       710\n",
      "           2      0.995     0.817     0.897       760\n",
      "\n",
      "    accuracy                          0.873      2400\n",
      "   macro avg      0.884     0.875     0.873      2400\n",
      "weighted avg      0.886     0.873     0.874      2400\n",
      "\n",
      "[[802 127   1]\n",
      " [ 37 671   2]\n",
      " [ 66  73 621]]\n",
      "=== 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 41:24, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.574630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.578800</td>\n",
       "      <td>0.503523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.335807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.300465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0.309473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.298001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.280431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.277809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.269583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.276078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.269555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.261685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.268045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.277476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.258128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.253857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.257856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.252925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.2529246509075165).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56457be057da486c8db960e40188f87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.936     0.822     0.875       930\n",
      "           1      0.775     0.992     0.870       710\n",
      "           2      0.991     0.882     0.933       760\n",
      "\n",
      "    accuracy                          0.891      2400\n",
      "   macro avg      0.901     0.898     0.893      2400\n",
      "weighted avg      0.906     0.891     0.892      2400\n",
      "\n",
      "[[764 160   6]\n",
      " [  6 704   0]\n",
      " [ 46  44 670]]\n",
      "=== 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='512' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [512/600 37:22 < 06:26, 0.23 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.650500</td>\n",
       "      <td>0.591025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.525015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.346521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>0.334163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.298312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.282972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.344948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.266616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.289200</td>\n",
       "      <td>0.279140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.292363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>0.265721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.268955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>0.260315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.270025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.265779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.263445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-416 (score: 0.26031485199928284).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bac3af152b4e8db1eb56e8e449de0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.811     0.985     0.889       930\n",
      "           1      0.930     0.807     0.864       710\n",
      "           2      1.000     0.861     0.925       760\n",
      "\n",
      "    accuracy                          0.893      2400\n",
      "   macro avg      0.914     0.884     0.893      2400\n",
      "weighted avg      0.906     0.893     0.893      2400\n",
      "\n",
      "[[916  14   0]\n",
      " [137 573   0]\n",
      " [ 77  29 654]]\n",
      "=== 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/600 41:04 < 01:43, 0.23 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.562563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.398677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.360300</td>\n",
       "      <td>0.314293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>0.283137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>0.289964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.285585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.277197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.292219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.277877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.269716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.267425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.264761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.251178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.247861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.239532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.244989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.242622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>0.250196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-480 (score: 0.23953185975551605).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53144cb1283645f9ba79f4d9cde1906d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.847     0.886       930\n",
      "           1      0.794     0.982     0.878       710\n",
      "           2      0.996     0.883     0.936       760\n",
      "\n",
      "    accuracy                          0.898      2400\n",
      "   macro avg      0.906     0.904     0.900      2400\n",
      "weighted avg      0.910     0.898     0.900      2400\n",
      "\n",
      "[[788 139   3]\n",
      " [ 13 697   0]\n",
      " [ 47  42 671]]\n",
      "=== 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/600 43:24 < 01:48, 0.22 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.581916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.527453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.311947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.315662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.297150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.315281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.295403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.280424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.297451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.306409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.276718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.277821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.267301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.269154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.251721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.252632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.254274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.258136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-480 (score: 0.2517211139202118).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aed68a204f84cc19067245975081890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.816     0.989     0.895       930\n",
      "           1      0.937     0.811     0.869       710\n",
      "           2      1.000     0.866     0.928       760\n",
      "\n",
      "    accuracy                          0.897      2400\n",
      "   macro avg      0.918     0.889     0.897      2400\n",
      "weighted avg      0.910     0.897     0.898      2400\n",
      "\n",
      "[[920  10   0]\n",
      " [134 576   0]\n",
      " [ 73  29 658]]\n",
      "=== 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/600 26:55 < 15:13, 0.24 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>0.579159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.597800</td>\n",
       "      <td>0.514366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>0.319483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.291635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.291046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.308114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.281224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.304418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.278680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.284896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.281292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.279573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-288 (score: 0.2786800265312195).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ae5fbdd07146a3bee44e0da13f4fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.873     0.902     0.887       930\n",
      "           1      0.801     0.921     0.857       710\n",
      "           2      0.998     0.818     0.899       760\n",
      "\n",
      "    accuracy                          0.881      2400\n",
      "   macro avg      0.891     0.881     0.881      2400\n",
      "weighted avg      0.892     0.881     0.882      2400\n",
      "\n",
      "[[839  90   1]\n",
      " [ 56 654   0]\n",
      " [ 66  72 622]]\n",
      "=== 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 43:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.563616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.376639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>0.316678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>0.298435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0.301671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.280210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>0.283612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.284474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.270084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.274974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.264732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.267284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>0.266113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.247508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>0.250021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.248599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.244474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.243821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.24382087588310242).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5aa7e1927c480c941980b62d60b07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.822     0.879       930\n",
      "           1      0.772     0.999     0.871       710\n",
      "           2      1.000     0.886     0.939       760\n",
      "\n",
      "    accuracy                          0.894      2400\n",
      "   macro avg      0.906     0.902     0.896      2400\n",
      "weighted avg      0.911     0.894     0.896      2400\n",
      "\n",
      "[[764 166   0]\n",
      " [  1 709   0]\n",
      " [ 44  43 673]]\n",
      "=== 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/600 19:01 < 25:45, 0.22 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.554196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.527205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>0.385044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.359700</td>\n",
       "      <td>0.306675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.288761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.295932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.318113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.313276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-160 (score: 0.2887610197067261).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a745b86dcb74d6aaf7765796cdfe0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.896     0.849     0.872       930\n",
      "           1      0.756     0.961     0.846       710\n",
      "           2      1.000     0.811     0.895       760\n",
      "\n",
      "    accuracy                          0.870      2400\n",
      "   macro avg      0.884     0.874     0.871      2400\n",
      "weighted avg      0.887     0.870     0.872      2400\n",
      "\n",
      "[[790 140   0]\n",
      " [ 28 682   0]\n",
      " [ 64  80 616]]\n",
      "=== 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/600 19:58 < 21:47, 0.24 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.537468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.573400</td>\n",
       "      <td>0.451263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>0.333506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.355100</td>\n",
       "      <td>0.294342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.316676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.285732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.291306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.312331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.286335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-192 (score: 0.2857324182987213).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761f926d40a948f9a44ae72c2d08e2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.839     0.951     0.891       930\n",
      "           1      0.844     0.861     0.852       710\n",
      "           2      0.997     0.816     0.897       760\n",
      "\n",
      "    accuracy                          0.881      2400\n",
      "   macro avg      0.893     0.876     0.880      2400\n",
      "weighted avg      0.890     0.881     0.882      2400\n",
      "\n",
      "[[884  46   0]\n",
      " [ 97 611   2]\n",
      " [ 73  67 620]]\n",
      "=== 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 43:13, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.641900</td>\n",
       "      <td>0.624653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.569300</td>\n",
       "      <td>0.461525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>0.343815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.358800</td>\n",
       "      <td>0.306233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>0.280553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.295600</td>\n",
       "      <td>0.333829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.296202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.288000</td>\n",
       "      <td>0.276606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.266942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.271317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.272613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.260252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.253546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.251132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.229100</td>\n",
       "      <td>0.257467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>0.246519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.242814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.247407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-544 (score: 0.24281428754329681).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c413fe7ecdc24d0b972622c0f9aaeb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.908     0.856     0.881       930\n",
      "           1      0.798     0.963     0.873       710\n",
      "           2      1.000     0.876     0.934       760\n",
      "\n",
      "    accuracy                          0.894      2400\n",
      "   macro avg      0.902     0.899     0.896      2400\n",
      "weighted avg      0.904     0.894     0.895      2400\n",
      "\n",
      "[[796 134   0]\n",
      " [ 26 684   0]\n",
      " [ 55  39 666]]\n",
      "=== 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 40:27, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.646500</td>\n",
       "      <td>0.555126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.393508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.301812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.310683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.293434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.305496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>0.270221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.277028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.267026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.271094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.270384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.260818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.268755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.256571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.261661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>0.254033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.259610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.252587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-64\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-64/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-64/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-32] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-96\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-96/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-96/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-64] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-128\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-128/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-128/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-160\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-160/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-96] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-192\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-192/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-192/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-224\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-224/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-224/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-160] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-256\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-256/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-256/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-288\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-288/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-288/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-224] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-320\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-320/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-320/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-352\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-352/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-352/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-384\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-384/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-288] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-352] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-416\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-416/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-416/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-448\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-448/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-384] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-416] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-480\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-480/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-480/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-512\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-512/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-448] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-544\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-544/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-544/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-576\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-576/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-512] due to args.save_total_limit\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-544] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints2-xlmr-mr/checkpoint-576 (score: 0.25258687138557434).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a7b64e583a4c90af23a8ad3ffe6b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.898     0.858     0.877       930\n",
      "           1      0.796     0.955     0.868       710\n",
      "           2      1.000     0.867     0.929       760\n",
      "\n",
      "    accuracy                          0.890      2400\n",
      "   macro avg      0.898     0.893     0.891      2400\n",
      "weighted avg      0.900     0.890     0.891      2400\n",
      "\n",
      "[[798 132   0]\n",
      " [ 32 678   0]\n",
      " [ 59  42 659]]\n",
      "=== 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-large/resolve/main/config.json from cache at /home/altsoph/.cache/huggingface/transformers/4d7a1550c9ab8701667bc307a1213c040fcc06dc87a5e4994e72aecc0d7e0337.302e267433fe7c84959a639e9c7c555043daa4020c0daf311785b53de7b8685e\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /home/altsoph/.cache/huggingface/transformers/dc0198bb42e28700de2a550508894cf6c5202c38c7aff44b71a055950dfc2f99.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer.json from cache at /home/altsoph/.cache/huggingface/transformers/7766c86e10505ed9b39af34e456480399bf06e35b36b8f2b917460a2dbe94e59.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file checkpoints/checkpoint-256/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at checkpoints/checkpoint-256.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁no', '▁big', '▁prostitu', 'te', '[NOT]', '▁brush', 'es', '▁no', '▁flori']\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 19200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 38/600 02:27 < 38:24, 0.24 it/s, Epoch 0.25/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.657400</td>\n",
       "      <td>0.564354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints2-xlmr-mr/checkpoint-32\n",
      "Configuration saved in checkpoints2-xlmr-mr/checkpoint-32/config.json\n",
      "Model weights saved in checkpoints2-xlmr-mr/checkpoint-32/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints2-xlmr-mr/checkpoint-576] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "for run in range(40):\n",
    "    print('===',run)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "    tokenizer.add_tokens(['[NOT]','[FEW]','[MANY]'], special_tokens=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('checkpoints/checkpoint-256', num_labels=len(labels))\n",
    "    model = model.to(\"cuda\")\n",
    "    model.resize_token_embeddings(len(tokenizer))    \n",
    "    \n",
    "    np.random.shuffle(train_records2)\n",
    "    train_data2 = NLIsDataset(train_records2, tokenizer, MAX_TOKENS, labels)\n",
    "    val_data2 = NLIsDataset(val_records2, tokenizer, MAX_TOKENS, labels)\n",
    "\n",
    "    print(tokenizer.convert_ids_to_tokens(val_data2[0]['input_ids'])[:10])\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"checkpoints2-xlmr-mr\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        logging_steps=EVAL_STEPS,\n",
    "        save_steps=EVAL_STEPS,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data2,\n",
    "        eval_dataset=val_data2,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    !rm -rf checkpoints2\n",
    "    trainer.train()    \n",
    "    \n",
    "    model.eval()\n",
    "    pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, framework=\"pt\", device=0, return_all_scores=True)\n",
    "\n",
    "    y_true2 = np.array([labels.index(r[\"label\"]) for r in test_records2], dtype=np.int32)\n",
    "    test_pairs2 = [(r[\"left\"], r[\"right\"]) for r in test_records2]\n",
    "\n",
    "    y_pred2, y_pred_prob2 = pipe_predict(test_pairs2, pipe)\n",
    "\n",
    "    print(classification_report(y_true2, y_pred2, digits=3))\n",
    "    print(confusion_matrix(y_true2, y_pred2))    \n",
    "    \n",
    "    not_embd = model.roberta.embeddings.word_embeddings.weight.data[[tokenizer.vocab['[NOT]']]] # .cpu().detach().numpy())\n",
    "    few_embd = model.roberta.embeddings.word_embeddings.weight.data[[tokenizer.vocab['[FEW]']]]\n",
    "    many_embd = model.roberta.embeddings.word_embeddings.weight.data[[tokenizer.vocab['[MANY]']]]\n",
    "\n",
    "    np.savetxt(f'embd.not.xlmr.{run:02}.txt', not_embd.cpu().detach().numpy())\n",
    "    np.savetxt(f'embd.few.xlmr.{run:02}.txt', few_embd.cpu().detach().numpy())\n",
    "    np.savetxt(f'embd.many.xlmr.{run:02}.txt', many_embd.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNrG+MkaPbNGzb8ySmY1mL3",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HeadlineCause.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0157731fa2bc462f869e00544ddd24d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_954283fe5e0e415b9845f216cc6819b7",
       "IPY_MODEL_5da582397fbf4680ba6e556d8a889090",
       "IPY_MODEL_981fc4580f65438496be46ec2831c01d"
      ],
      "layout": "IPY_MODEL_bb7d334cf7424f1f87d8a6b3c87416e3"
     }
    },
    "0237d6c8e31b4cb88efc3c554fa503e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6643e46efdb7407684b825dcbc96a6b0",
      "max": 7347,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d9bf8111fdf4473a0214e73c8f0904f",
      "value": 7347
     }
    },
    "0274a05891e342848be192f8046244b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_06f50b846e0b4d798f9539d231d8d73e",
       "IPY_MODEL_8f88412ed9d242af9be0fec29653e18f",
       "IPY_MODEL_3832ea9863dd45deb009574adcc157b9"
      ],
      "layout": "IPY_MODEL_7e1ea7d4b4454449bec0960459cc9f53"
     }
    },
    "028607648b6546cbaec8cd737fb5f584": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03908fdee9b646ae8074f28d864f933b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a84c592c55448b88ff4d4973c451ce6",
       "IPY_MODEL_97ca56c9528e41e3af81fffea441bd4c",
       "IPY_MODEL_e34c2879a0c94f468c41b7896b237d90"
      ],
      "layout": "IPY_MODEL_db4da210dd184d53a2c92a57671c7d79"
     }
    },
    "03ac9a49e4774957b3c603ab4a7b0f5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03d8bee45e714172852ecb9da7993627": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "040a43c0a80749aa91c84e60c9dc9df1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0444bbfb1d5540c8a861fbaa402623a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "045560d87382405097639dde6aab8468": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "055bffbf4c6e45ec8c64af3516387d56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09d31effa8eb47cead0c7c2694a7081a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a1dd233b8ec4c869d459611281790e6",
      "value": 1
     }
    },
    "05bd42bf76cf4a8bb149287688d320be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0682fece9af849e5a74231002cb52a5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06a8f4e813d14484aae07191437755b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06f50b846e0b4d798f9539d231d8d73e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be9589537dcf44dd8c85d4b624ace520",
      "placeholder": "​",
      "style": "IPY_MODEL_5322daad6f874e7ba48f6bf62b01759d",
      "value": "Downloading: 100%"
     }
    },
    "07a6a4a0e60f43af9f38a666210f1408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08fceb90f22b4db78f6ce0bed8f4e207": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0900ef4eae0b44239dd315e39b327bb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "096aeabf0e7447edbe88d9861a6d9df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09d31effa8eb47cead0c7c2694a7081a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "0da89b7fdaa24b20891b354d5acf11d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2076819162704754ab115426184105e8",
       "IPY_MODEL_4134f37b33534323b6d1d782e4a084c1",
       "IPY_MODEL_e411978c571e4f5ba1e99c52829884c9"
      ],
      "layout": "IPY_MODEL_14ec36342ba447ab83761b0cd8c241c8"
     }
    },
    "0de17ed106e44ce5a42943edc189fcb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3712a099dd994bbaa077cbd4228b3e40",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91301ccaa8a44b84a5c1b2b40a30b1a5",
      "value": 1
     }
    },
    "0e266648420c4ea6a1d33677099e825c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be60f1ffaa5c4a28949245b894002faa",
      "max": 5069051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec85a89cfab64e17a7ed792bbe77ad10",
      "value": 5069051
     }
    },
    "0e2dec2610a2414488020a5d1d0ea88a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e6361baca37459182dbbf8bf62d72a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b134bd89f6ea43f1a1f8b67973732c8d",
      "placeholder": "​",
      "style": "IPY_MODEL_31fbe900180742119fb0ccdce059f35e",
      "value": " 4/? [00:01&lt;00:00,  2.41it/s]"
     }
    },
    "0f4d3131dd564abdbc824e4bc9e2928c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fe466b954204ecd9d33caddf248da52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "104d1062a1b24e29909e86395b7f76f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11c6f20f4ee248d290b735557b5415ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12221b5946bf4b2ab5a029ffd8a07454": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12263828a3484fd2a40126a2a99096ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12456134939949f7946bb948cc2bd93c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed8804207e074fc5a09f9d59cfbfe2fa",
      "placeholder": "​",
      "style": "IPY_MODEL_e55f529fc04942109243e8a274c36d56",
      "value": " 9/? [00:06&lt;00:00,  1.52it/s]"
     }
    },
    "1327d7b1239f428394c9473fe3a092c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14ec36342ba447ab83761b0cd8c241c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14f924f3402040a08ea05648cc59fd72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16565a3ef2234725b25eb605e7212c22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9371eb27783a42de9846837f448b2634",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dd29333790d9487d92b482cd0a9fcc60",
      "value": 1
     }
    },
    "17e1c1d3b74843ba8e0b184ed872c724": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38e734fe318b41c6ad2a6817b11735c6",
      "placeholder": "​",
      "style": "IPY_MODEL_06a8f4e813d14484aae07191437755b8",
      "value": ""
     }
    },
    "1946cd591496468895125e06f5d1f8c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39796b9134094247a48ce160a6550fe6",
      "placeholder": "​",
      "style": "IPY_MODEL_b20668b9b7b2442e9b5aff51efb26e5a",
      "value": "Downloading: 100%"
     }
    },
    "1a84c592c55448b88ff4d4973c451ce6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73a87bcaebcb4390962a62b273c91088",
      "placeholder": "​",
      "style": "IPY_MODEL_b883757a551f480e8c5ecb911491ec76",
      "value": "Downloading: 100%"
     }
    },
    "1c3ba1e8c956425a98342a9c57c5afff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c01613112a194d7290eb2ad1736ecc3b",
       "IPY_MODEL_e61231528fe34c3cb842add015f114ad",
       "IPY_MODEL_bcf3045fc06f42dda3236ebd9d00e12c"
      ],
      "layout": "IPY_MODEL_aae3d41ae7c8454181fbd20afcf27940"
     }
    },
    "1c6067a81f7a4e1cb7123c5e6b33be4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d045d76cf5744c99fdbdd3a9f5d278a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78b3937753b844b5914918ec7b1e0245",
      "placeholder": "​",
      "style": "IPY_MODEL_3e9d01b2d70f4571be96377f4063ee8d",
      "value": ""
     }
    },
    "1d1429d702b5408480f6cdf8094f4b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e5d9ca735054147ac3a856e4abf61a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e9f3e09c4a0467283eb1ffd374f67fc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07a6a4a0e60f43af9f38a666210f1408",
      "value": 1
     }
    },
    "1f2c04f83cdd4dcd826f122f2dbd50c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63f20b38f41e4d5bbbc5b1ea772ea354",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a6649aa1a184e7393d263142041ea14",
      "value": 1
     }
    },
    "1f46d337ae324d17aaca039402af302f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33248311785e4710a88003f6fab0e5ff",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0c958294b364af8b07913eeb2645d7e",
      "value": 1
     }
    },
    "2076819162704754ab115426184105e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c93c197f924244e1ace188a3b9a4d70d",
      "placeholder": "​",
      "style": "IPY_MODEL_88474c796c1b4471913e15708b9d95b8",
      "value": ""
     }
    },
    "20d84a687280490d8babe216ccf2b0bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1de70e801584e3bb4e9269d206f8df2",
      "max": 9096718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45e4ddc0bffc41b79e5c475b05b5d9f3",
      "value": 9096718
     }
    },
    "2208d46b54d243b087e48449299a74eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "22854f25cbae42b3884d9c15d2b1c86d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5db6170f4bb4461a57de21d67cea869",
       "IPY_MODEL_f825d1691d9941b19dc9e3c8f1f09a9a",
       "IPY_MODEL_dbc80c4cb096434d87d0bdeaa961bd22"
      ],
      "layout": "IPY_MODEL_e36d0e0128d8427b9f7d3fcfac59ff1a"
     }
    },
    "22887b1da48f4efe91d70d37b30ecc88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23ae2137cb24411a98f2989c18046d7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23ba7435a2df4698b48c88b73327c0d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23cda8e4645747bbaf17201f184f70a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2425de3039334c3a90941aa59e092a12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b79225710cee4015894c83d388c3e673",
      "placeholder": "​",
      "style": "IPY_MODEL_4254651428ec41548aa59a1ffc90a382",
      "value": " 9/? [00:05&lt;00:00,  1.76it/s]"
     }
    },
    "2448dc0e3d26474a8d474232535c55b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "257858d94c3a4a3da29d81dccb023ebd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_028607648b6546cbaec8cd737fb5f584",
      "placeholder": "​",
      "style": "IPY_MODEL_a7a4b53ef5524deea80b444f52acb135",
      "value": " 4/? [00:01&lt;00:00,  3.02it/s]"
     }
    },
    "259572bb29e34d059192c8b7cf17b4dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25a6787bfe76485f80986e0dbd9be1f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "262e6afaf5da41a5976916b55c786add": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "27e6d106679d4fddb3aa31ba458f1087": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29c44eabeaf547a184f70d02c9e9b6b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a051ae5a8ef4f62a941b2e3892c6a38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2aa88a8c258142ebbc5a843634224474": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b4fd8bac4a74d1bbd244e69678d7aa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12221b5946bf4b2ab5a029ffd8a07454",
      "placeholder": "​",
      "style": "IPY_MODEL_3eb3e2a357104b8a8fcd88a10a5a8fd6",
      "value": "100%"
     }
    },
    "2b628b1c30694495b3cb0eeeea09eff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b79a72eff5444bba4dc8ed0b8488c0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81aef0cce7d44253800cb502602135e4",
      "max": 513,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9a66256a7224d5e8674fbca5703c175",
      "value": 513
     }
    },
    "2b902c865ccf43f0a9e2ae819391a0c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c55cede827941c3aa2b167db6241912": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e1e8203e2d64e5b8d961bb8800b83b3",
      "placeholder": "​",
      "style": "IPY_MODEL_e0a7a546187148b299ef648197f0853f",
      "value": ""
     }
    },
    "2c62f3b516b840acbf38176e7dea8701": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cb97ad3be7340cdad444adc4097f1e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ade8aba81d34f4f8d55e7c0e0c9fd08",
      "placeholder": "​",
      "style": "IPY_MODEL_8344d7c63f844cd6af5f30f1f914f505",
      "value": ""
     }
    },
    "2d372d8c558944f68c5d831efe66e709": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a1cae88538a54a7a9fd225875b5efe12",
       "IPY_MODEL_36240cb00ee74b86b6183225475920e1",
       "IPY_MODEL_c24c279e4c74429f9f0ddbb2f7b4b94b"
      ],
      "layout": "IPY_MODEL_3c46d5daff684c9dafcf257c40a1c33a"
     }
    },
    "2d683ea3f6bc481788f17a2eb98f8de2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "2d799e14664e4f71a0e52d002603ae6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4465fde1782d4ae5931a9e2d1cfdd5ef",
      "placeholder": "​",
      "style": "IPY_MODEL_a62dafcccde341ddaa23efc711f4baef",
      "value": " 4/? [00:02&lt;00:00,  1.41it/s]"
     }
    },
    "2d9df97006eb4029b08e85fde32aca56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e43d6a5eff446c994fd7b491b2fba55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a7fdfa2ad9b34b739b00741fea65ef60",
       "IPY_MODEL_94ecd667232a402b825f017a0318af8b",
       "IPY_MODEL_ea48400f02c64d6a94277b81c57e2885"
      ],
      "layout": "IPY_MODEL_c6aa5349d29840fbaede343ae9c1394d"
     }
    },
    "2f7f72a029344319af2f7e4662d9d9c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ff8491756f045ae980f1b17aa0de59f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "305c662c690d4a8fbdd20fb9727270b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30a92e3e16bf4aa09cde0a794ee99273": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30ebffabcd2843dab3b47a66f4cacfbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_430bac30e54e4475a3d06f2ae40eb8f2",
      "placeholder": "​",
      "style": "IPY_MODEL_fd5c87f273364d6b8b01c7ae0d0d8d60",
      "value": " 5.07M/5.07M [00:00&lt;00:00, 4.71MB/s]"
     }
    },
    "3160f691bf8649d599c5beb5edf16e6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31f8640659b74b23a7a2467a0dbb9696": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "31fbe900180742119fb0ccdce059f35e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3280994730964cc39b553bef74010eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7405e8f8a7654ccbad6cdbe4ef9e2d3f",
       "IPY_MODEL_2b79a72eff5444bba4dc8ed0b8488c0e",
       "IPY_MODEL_eb035aff4e47418b88f0f41db8388725"
      ],
      "layout": "IPY_MODEL_a5bad952f6dd4a8e80069cd6184aa42f"
     }
    },
    "328348c968b142d38b627033364aea3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33248311785e4710a88003f6fab0e5ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "337003cadee2442ba59494dd6e3131ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "339c896eb8bf470f8161a519894d727a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33b2025ec87c4d1eb666085053cc5250": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35055fc27219499b9160509c670bfbb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "357777cea3fa4abc935f901dbdf1bdf2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "359143f61ed24f8481f13480092c5405": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35cb017075724474ad6746745dbd0b24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35e4c77a71e54790a57065fb22c5403b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "36240cb00ee74b86b6183225475920e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78e485c399064618b6ad60730f9b2403",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78c7cc6c3a024a189bfbc7044f89a339",
      "value": 1
     }
    },
    "3634916312a347fe9e6ee9c1d59b770f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5b3c53eb92e4e37aaa171b092828eb2",
      "placeholder": "​",
      "style": "IPY_MODEL_9c9c9944454e4370b4426cd97d51af77",
      "value": ""
     }
    },
    "3661a1ad8bbc45a691b4a7c702b5a972": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "3712a099dd994bbaa077cbd4228b3e40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "3832ea9863dd45deb009574adcc157b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c5af5a0ea534b61a23084b5579e3fcb",
      "placeholder": "​",
      "style": "IPY_MODEL_35055fc27219499b9160509c670bfbb4",
      "value": " 2.24G/2.24G [01:08&lt;00:00, 28.3MB/s]"
     }
    },
    "38e734fe318b41c6ad2a6817b11735c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39796b9134094247a48ce160a6550fe6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39c502e381984ee69805b8959f51aec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_637220cc21214f7c8cf8e4e3444a7fd7",
      "placeholder": "​",
      "style": "IPY_MODEL_2aa88a8c258142ebbc5a843634224474",
      "value": ""
     }
    },
    "39eaa7ab6d204e5ca4aa4f50b809312f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_305c662c690d4a8fbdd20fb9727270b7",
      "placeholder": "​",
      "style": "IPY_MODEL_ca4167d7ae094dd3b0ff00c283204750",
      "value": ""
     }
    },
    "3a380f3aeff8446581dbee64a6b4609f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3a59fe30158b42989ebb093dd0d7d44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b7e34dfd0474b4180277c0260dddca2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0a82b71d800496e8ca6caa0f8c6118f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_040a43c0a80749aa91c84e60c9dc9df1",
      "value": 1
     }
    },
    "3b852c1ccd144b298f87078d46790105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22887b1da48f4efe91d70d37b30ecc88",
      "placeholder": "​",
      "style": "IPY_MODEL_9e4c8e4c1b8d42c2a03dba54f5c023a4",
      "value": " 15/? [00:10&lt;00:00,  1.65it/s]"
     }
    },
    "3c46d5daff684c9dafcf257c40a1c33a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c7fcc0b4def4a198a631e9ea9683a43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ce8029bc0cd4b1890d458ac51d8fc2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f020c9f836c4b0cbfd1af64855e2793",
      "placeholder": "​",
      "style": "IPY_MODEL_3ceca586832a4d0c80ca4d1b709657bd",
      "value": " 4/? [00:01&lt;00:00,  2.01it/s]"
     }
    },
    "3ceca586832a4d0c80ca4d1b709657bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d7648a85843493f9f5ffd83161edcf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d9bf8111fdf4473a0214e73c8f0904f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e34dfdace3a49388040b30a43a0d44e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "3e9d01b2d70f4571be96377f4063ee8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3eb3e2a357104b8a8fcd88a10a5a8fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f02c2320b514d519b23112088f429c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "3f861aad8dc9474f84e3f7d5836c37d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4084d9417542497089d7d9d3a9d4d971": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40bc13848dcb459392359e060ec053c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40f728677716424c831f61d50cccfb87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88b1d5296f8544949906402844bca853",
      "max": 5069051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03d8bee45e714172852ecb9da7993627",
      "value": 5069051
     }
    },
    "4134f37b33534323b6d1d782e4a084c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d736ab979c6f4f52bd79dfac8859ce9a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b888f5961e444430b6bc3b163bfa1741",
      "value": 1
     }
    },
    "414962b849174a3fbfe604637f2b21c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41771e47c8484faca97600bc6a8db761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4254651428ec41548aa59a1ffc90a382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "426a6b3aafdd497f8e2bc9a6e30e0c0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42cd3cce293040368855cfdc7b5e1154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "430bac30e54e4475a3d06f2ae40eb8f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "432277a1a34c4e40aa0c46ed5f789f69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43bd340459f54bab9b7e030e6244075b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4429c9854f564f11815f9725ca9816cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1946cd591496468895125e06f5d1f8c3",
       "IPY_MODEL_20d84a687280490d8babe216ccf2b0bf",
       "IPY_MODEL_a124de9d40914ac193147bfc2301ca98"
      ],
      "layout": "IPY_MODEL_74f601bbfe8d46058339168d18ac85f2"
     }
    },
    "4448c07bd5ce48c78308283e5960c394": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9a5b71477dc4dde9872bc2812b6ee1a",
      "placeholder": "​",
      "style": "IPY_MODEL_55d6e3222b1144fc93ea86f100c5a05d",
      "value": " 513/513 [00:00&lt;00:00, 13.7kB/s]"
     }
    },
    "4465fde1782d4ae5931a9e2d1cfdd5ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45e4ddc0bffc41b79e5c475b05b5d9f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "45ed01535f25403c8cd2f00bdb347c5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc004c8f2b3441cd8d395c9f7778dd3b",
       "IPY_MODEL_9dd37fcca3fc4a6db18785c80e1c5f78",
       "IPY_MODEL_2425de3039334c3a90941aa59e092a12"
      ],
      "layout": "IPY_MODEL_7d40c1f0ca8e4e179345f7f51d00e274"
     }
    },
    "478ad9c8da7544059d3cdcccd0b4040d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "47bfd91a954d4313b8439a1251abece7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "495fbf6c5bf64517b0e029e4caeb500e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6af46d7f33ae4540a63eb7809801d202",
       "IPY_MODEL_0de17ed106e44ce5a42943edc189fcb6",
       "IPY_MODEL_257858d94c3a4a3da29d81dccb023ebd"
      ],
      "layout": "IPY_MODEL_40bc13848dcb459392359e060ec053c8"
     }
    },
    "4a3312b4fff049528a238aaf225f967f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ade8aba81d34f4f8d55e7c0e0c9fd08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b0ced42a14c46e7a999cfe4f0d60292": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b57654c984b4ba6b1994779a7b5de79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96db4ba22cab41629d59b29c629b2b2e",
      "placeholder": "​",
      "style": "IPY_MODEL_9686db28a028404cbb08a54461683207",
      "value": " 25/? [00:14&lt;00:00,  2.16it/s]"
     }
    },
    "4bd0b87187db46aebf0f07e257d0da66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4c2ac7ce415e471aa13297c53a11bd59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cb06a9d33b74f8d81d143aa3fe5f35b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4cb39f1096f845408dac2c8114152f1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6c9089de303c4f4e84f6345064f5ee6d",
       "IPY_MODEL_9cea9640eee14905bf00876b3be47663",
       "IPY_MODEL_7e8afeacef204fcca5e7f54e60060f5e"
      ],
      "layout": "IPY_MODEL_2ff8491756f045ae980f1b17aa0de59f"
     }
    },
    "4cd09071c48d4006aa68bc82dc75d282": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c6104df63cb41078131d32f779b1284",
      "placeholder": "​",
      "style": "IPY_MODEL_42cd3cce293040368855cfdc7b5e1154",
      "value": " 17/? [00:12&lt;00:00,  1.46it/s]"
     }
    },
    "4cfbb061ad3841a3bc783c55da399ed9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d5a68d6937347bcafe23ff3567b7528": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "4dbdff8de90549f3a3fd928c9a7e526a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39c502e381984ee69805b8959f51aec0",
       "IPY_MODEL_1e5d9ca735054147ac3a856e4abf61a3",
       "IPY_MODEL_e3b1e3717522402ab689cf386438700c"
      ],
      "layout": "IPY_MODEL_8c74a28b20414789945a017f47550f4f"
     }
    },
    "4e1e8203e2d64e5b8d961bb8800b83b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f020c9f836c4b0cbfd1af64855e2793": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4facfe6fe52546d383fee5342433b6e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1f613618d1e46749425bd8f1f841162",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b7284b074daf4f32ba02d4d95d905307",
      "value": 1
     }
    },
    "4fb8aa04be2b406ea085b9385875fc98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fcb2693d3214dbb8175394fc0d96e15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50ec086f5d3f44d98c922ea673db2464": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "51323bde0a324206ba1ee5a8b6e35c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52ac456567384c2da0e97b4daf6a4479": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5322daad6f874e7ba48f6bf62b01759d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "536d0ca8ad3240babc1a747848383813": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53cc42ffd95b4826ac3591566a81d11e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "55895ac82edb4fe9a24a0a0bc7b4c894": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55d6e3222b1144fc93ea86f100c5a05d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "570af36eef574e41bd33699b8976a503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58688a4e13584afbaea881a2cf7cec10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2cb97ad3be7340cdad444adc4097f1e3",
       "IPY_MODEL_7d22e6ebdc114497ada4605de2c58d59",
       "IPY_MODEL_6bf1c174ac9b4495b3a066591beada2b"
      ],
      "layout": "IPY_MODEL_1327d7b1239f428394c9473fe3a092c8"
     }
    },
    "58db907bec25432a9cbc04aeec1c205e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dabc1eed01e74ea293c943bfb9ad14ac",
      "placeholder": "​",
      "style": "IPY_MODEL_6eff7b047fea4b999d7de6f2481ae768",
      "value": ""
     }
    },
    "58dc1e3b108c448184f4516391d6750c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddd1e0f5b5b14d77be0c0fa9d3dc53de",
      "placeholder": "​",
      "style": "IPY_MODEL_d6ac3035f4fc46bcbb80f6952a982884",
      "value": " 16/? [00:05&lt;00:00,  3.05it/s]"
     }
    },
    "5a42a8d6fb204c0b9e1e7cfd5de6b3ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a051ae5a8ef4f62a941b2e3892c6a38",
      "placeholder": "​",
      "style": "IPY_MODEL_e9029752740946fa9e77b19fcf840dc6",
      "value": " 16/? [00:05&lt;00:00,  3.20it/s]"
     }
    },
    "5c2ab7d20b7e43eeb5faa1111f9370c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c06189f026484596922c5099d6be1b8e",
       "IPY_MODEL_0e266648420c4ea6a1d33677099e825c",
       "IPY_MODEL_30ebffabcd2843dab3b47a66f4cacfbd"
      ],
      "layout": "IPY_MODEL_f1e60bf7a6ab4f949965047e901543af"
     }
    },
    "5c5aacc923c643348d11eac30c81b658": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c6104df63cb41078131d32f779b1284": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cda19785cb44507bd3ff1a45f270604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25a6787bfe76485f80986e0dbd9be1f6",
      "placeholder": "​",
      "style": "IPY_MODEL_eace8a4075764b62889cc33dd1dec4f1",
      "value": ""
     }
    },
    "5da582397fbf4680ba6e556d8a889090": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d683ea3f6bc481788f17a2eb98f8de2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc5821db3e4147e2bd749c0f8d450b37",
      "value": 1
     }
    },
    "60e685ab2e6e488f8b9e58a11ffe35f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "6166700b766343bf989bafc730a7d7ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61c88881a39140e998fb1c42fcf4353d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9fea575dbca487b9dd6b04a7bd4143a",
       "IPY_MODEL_aceab5333b98449b80b9b1568ac7aaea",
       "IPY_MODEL_0e6361baca37459182dbbf8bf62d72a0"
      ],
      "layout": "IPY_MODEL_d14309d78a76443c9221a664de5812e4"
     }
    },
    "61e6c0de564344129fb7b4979abc3666": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84119387961d479e97149d993b598e65",
      "placeholder": "​",
      "style": "IPY_MODEL_ae494952f2aa4bc88a94d92c3527daf1",
      "value": " 13/? [00:04&lt;00:00,  3.31it/s]"
     }
    },
    "637220cc21214f7c8cf8e4e3444a7fd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63f20b38f41e4d5bbbc5b1ea772ea354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "641d317f1043472ab3d5c25c62e0ae8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "647f595d92f040189ed7bdffe8855fa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae0a8f582cf3444d9f31daa01d303d46",
       "IPY_MODEL_9d75a33b23294e33be810ed45de2dc1b",
       "IPY_MODEL_9d95de89784d4775bfd62364bee6e9d9"
      ],
      "layout": "IPY_MODEL_14f924f3402040a08ea05648cc59fd72"
     }
    },
    "64fc347b02f94171b7cd22a186781e30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6556e820ac704722b7903254167e430f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6571bcd10bce453380f6d131bc2dea5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "663b2684137e46adb6fc54134c17b350": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_94e2fb640e6f48d594aee6b2fe20c408",
       "IPY_MODEL_b36eda2803b9411eb35691db39c59337",
       "IPY_MODEL_3b852c1ccd144b298f87078d46790105"
      ],
      "layout": "IPY_MODEL_23ae2137cb24411a98f2989c18046d7a"
     }
    },
    "6643e46efdb7407684b825dcbc96a6b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66cdea98ac614e69b0d1f5f8ac13f951": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e34dfdace3a49388040b30a43a0d44e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f81cc67befa4445969953c9d8b26a0f",
      "value": 1
     }
    },
    "671e90a0fffb44449a4c2ee5d93fec8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "67a26d9f5adb4278a236adca6c2677e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6839407e4cdf43159ecf9fa2cff97980": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6af46d7f33ae4540a63eb7809801d202": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33b2025ec87c4d1eb666085053cc5250",
      "placeholder": "​",
      "style": "IPY_MODEL_4c2ac7ce415e471aa13297c53a11bd59",
      "value": ""
     }
    },
    "6bf1c174ac9b4495b3a066591beada2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d9df97006eb4029b08e85fde32aca56",
      "placeholder": "​",
      "style": "IPY_MODEL_c4e300a8eb224cb3829d064b88261eea",
      "value": " 4/? [00:01&lt;00:00,  2.59it/s]"
     }
    },
    "6c9089de303c4f4e84f6345064f5ee6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76a7856dcc8647159182743e7a3dfaac",
      "placeholder": "​",
      "style": "IPY_MODEL_4084d9417542497089d7d9d3a9d4d971",
      "value": ""
     }
    },
    "6edeef790bef4ba8b78278afd32d7a14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39eaa7ab6d204e5ca4aa4f50b809312f",
       "IPY_MODEL_91e0078148344fae82f884a1bc2c6dac",
       "IPY_MODEL_4b57654c984b4ba6b1994779a7b5de79"
      ],
      "layout": "IPY_MODEL_986ec9501c4b49b6867751c3aba5d592"
     }
    },
    "6ef15d55c12d436d8fce0762c4dd7eca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eff7b047fea4b999d7de6f2481ae768": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f05584129c94c3184735a6e5f7a5e41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f98f87aa5df42f49389fe1830a18528": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f9f720e867b4e149809f64c276f70ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "71a477c368284e3eac967eff2f1b5f19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71e7bce851454a45ba053634f2a3fdf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "73a87bcaebcb4390962a62b273c91088": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7405e8f8a7654ccbad6cdbe4ef9e2d3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e1052de422846de94c443deeb1f4a19",
      "placeholder": "​",
      "style": "IPY_MODEL_bcc1a727668e4497a2e87b83d666ba84",
      "value": "Downloading: 100%"
     }
    },
    "74505852eed14afdbab8ef4906af3b1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74f601bbfe8d46058339168d18ac85f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "753482707ba74d16a09305a9e2af2f03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7596b6ec9d0840f6804bd06be12a5f0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "75b0dab2d0ee447c97900b821491459c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76a7856dcc8647159182743e7a3dfaac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77937db7e91549c6b6b8b3812e64e243": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78b3937753b844b5914918ec7b1e0245": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78c7cc6c3a024a189bfbc7044f89a339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78e485c399064618b6ad60730f9b2403": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7bdaf6a083a34503bb02195bad9f423d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d031bdcfd5f0452ca1d3104865fda313",
       "IPY_MODEL_40f728677716424c831f61d50cccfb87",
       "IPY_MODEL_7cdb378dc1e042c5bd755c8ab7549711"
      ],
      "layout": "IPY_MODEL_045560d87382405097639dde6aab8468"
     }
    },
    "7c4fc1f6a61d4c25adf706b170972c37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c90f659974b4f88be0a45d9bf0fa85e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7cc9055938dd4a7b80f02be3a0ac9c6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_337003cadee2442ba59494dd6e3131ac",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67a26d9f5adb4278a236adca6c2677e1",
      "value": 1
     }
    },
    "7cdb378dc1e042c5bd755c8ab7549711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96473d5230194558911ae691f658ee7a",
      "placeholder": "​",
      "style": "IPY_MODEL_aa75ea17cff24e78896e72770a75477e",
      "value": " 5.07M/5.07M [00:00&lt;00:00, 6.01MB/s]"
     }
    },
    "7d22e6ebdc114497ada4605de2c58d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_262e6afaf5da41a5976916b55c786add",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f9f720e867b4e149809f64c276f70ef",
      "value": 1
     }
    },
    "7d40c1f0ca8e4e179345f7f51d00e274": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d884bf7662245e0877fc76124b66e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6166700b766343bf989bafc730a7d7ab",
      "placeholder": "​",
      "style": "IPY_MODEL_432277a1a34c4e40aa0c46ed5f789f69",
      "value": " 4/? [00:01&lt;00:00,  3.09it/s]"
     }
    },
    "7e0bdeb0ab1d4f0284f9ea558cdce38f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23cda8e4645747bbaf17201f184f70a6",
      "placeholder": "​",
      "style": "IPY_MODEL_11c6f20f4ee248d290b735557b5415ce",
      "value": ""
     }
    },
    "7e1052de422846de94c443deeb1f4a19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e1ea7d4b4454449bec0960459cc9f53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e4261304a8d4b7dabf9f65ba70b7c6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e8afeacef204fcca5e7f54e60060f5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c78bb408f5474dedb9aeb36cf11fe353",
      "placeholder": "​",
      "style": "IPY_MODEL_ccb370cce221461a819e8e1d822e98a1",
      "value": " 30/? [00:18&lt;00:00,  1.81it/s]"
     }
    },
    "7e9f3e09c4a0467283eb1ffd374f67fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7f281043e69746c58e6a2788f0012c07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f414caddd514211ac49b52b58bfd77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81aef0cce7d44253800cb502602135e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822bccb4b6304bbab873c18aac29ddcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8344d7c63f844cd6af5f30f1f914f505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83a10f859d754220b4bb5894ad0ab92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_328348c968b142d38b627033364aea3a",
      "placeholder": "​",
      "style": "IPY_MODEL_850c74ac86324b63bbc305212502b51f",
      "value": " 12/? [00:07&lt;00:00,  1.77it/s]"
     }
    },
    "84119387961d479e97149d993b598e65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "846ec7db746f4adeba6e4672eef0891a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17e1c1d3b74843ba8e0b184ed872c724",
       "IPY_MODEL_055bffbf4c6e45ec8c64af3516387d56",
       "IPY_MODEL_7d884bf7662245e0877fc76124b66e84"
      ],
      "layout": "IPY_MODEL_c6db107c2e3946bd89b14bb34ee6b779"
     }
    },
    "850c74ac86324b63bbc305212502b51f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "858dfb6a7a1645328f19574d13f892a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cfbb061ad3841a3bc783c55da399ed9",
      "placeholder": "​",
      "style": "IPY_MODEL_426a6b3aafdd497f8e2bc9a6e30e0c0e",
      "value": ""
     }
    },
    "87ca4a68dee14897b6d7b122ee1bcd8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87fe6b1344f44136b7fddfdad0ab04a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88474c796c1b4471913e15708b9d95b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88b1d5296f8544949906402844bca853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88e96bca3f5a4b45a5881234e31df92c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88ed841e041443c397df4c060b0824a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcada27ceac24d758f6fc31a1ecbc944",
      "placeholder": "​",
      "style": "IPY_MODEL_29c44eabeaf547a184f70d02c9e9b6b2",
      "value": "100%"
     }
    },
    "8a1dd233b8ec4c869d459611281790e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b0f11d1ec034ec28d792b6b71644d11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c4c5e1e60884044bb7df1b115fa7f95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0900ef4eae0b44239dd315e39b327bb2",
      "placeholder": "​",
      "style": "IPY_MODEL_7e4261304a8d4b7dabf9f65ba70b7c6d",
      "value": " 7347/7347 [01:33&lt;00:00, 80.46it/s]"
     }
    },
    "8c74a28b20414789945a017f47550f4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8da4725509164a72b2552f54a8fb59ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f81cc67befa4445969953c9d8b26a0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f88412ed9d242af9be0fec29653e18f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c5aacc923c643348d11eac30c81b658",
      "max": 2244861551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2208d46b54d243b087e48449299a74eb",
      "value": 2244861551
     }
    },
    "91301ccaa8a44b84a5c1b2b40a30b1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "918f0931a4114ee09c6e30b7317114c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91e0078148344fae82f884a1bc2c6dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31f8640659b74b23a7a2467a0dbb9696",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6556e820ac704722b7903254167e430f",
      "value": 1
     }
    },
    "91e73e98616d4e4bbde76d00464556c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93593049b7a84bd4b802fac3598ff700": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93614bdaf09a4d199e88e956a4d442cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9362fc344e3b4a1a881a9d7a4718f3d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f861aad8dc9474f84e3f7d5836c37d9",
      "placeholder": "​",
      "style": "IPY_MODEL_4fb8aa04be2b406ea085b9385875fc98",
      "value": " 4/? [00:01&lt;00:00,  2.74it/s]"
     }
    },
    "9371eb27783a42de9846837f448b2634": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "943cf03c372241d0be5a6d3a4451af6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9e6bc8678d64dc58e01d6a1f5571ca7",
      "placeholder": "​",
      "style": "IPY_MODEL_ff4a53d7670a4021a9b8a6179b24a37c",
      "value": ""
     }
    },
    "94e2fb640e6f48d594aee6b2fe20c408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87fe6b1344f44136b7fddfdad0ab04a2",
      "placeholder": "​",
      "style": "IPY_MODEL_adca99c9ba6e47e399a9a96d4df13c1a",
      "value": ""
     }
    },
    "94ecd667232a402b825f017a0318af8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d5a68d6937347bcafe23ff3567b7528",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8da4725509164a72b2552f54a8fb59ae",
      "value": 1
     }
    },
    "954283fe5e0e415b9845f216cc6819b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_339c896eb8bf470f8161a519894d727a",
      "placeholder": "​",
      "style": "IPY_MODEL_a1cb2840011547199cd8ee370d2b3c50",
      "value": ""
     }
    },
    "96375517c09545c6802ec8b300699728": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96473d5230194558911ae691f658ee7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9686db28a028404cbb08a54461683207": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96db4ba22cab41629d59b29c629b2b2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97ca56c9528e41e3af81fffea441bd4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e966f67834bf4c5c930f75c3e2f66b56",
      "max": 9096718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51323bde0a324206ba1ee5a8b6e35c66",
      "value": 9096718
     }
    },
    "981fc4580f65438496be46ec2831c01d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3160f691bf8649d599c5beb5edf16e6a",
      "placeholder": "​",
      "style": "IPY_MODEL_570af36eef574e41bd33699b8976a503",
      "value": " 4/? [00:01&lt;00:00,  2.62it/s]"
     }
    },
    "986ec9501c4b49b6867751c3aba5d592": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a6649aa1a184e7393d263142041ea14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c5af5a0ea534b61a23084b5579e3fcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c9c9944454e4370b4426cd97d51af77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cea9640eee14905bf00876b3be47663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6bbc24e654240ccbad8d9502d93862b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_30a92e3e16bf4aa09cde0a794ee99273",
      "value": 1
     }
    },
    "9d75a33b23294e33be810ed45de2dc1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0444bbfb1d5540c8a861fbaa402623a4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a3e2921d5f734b48bb8bda271a827226",
      "value": 1
     }
    },
    "9d95de89784d4775bfd62364bee6e9d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91e73e98616d4e4bbde76d00464556c5",
      "placeholder": "​",
      "style": "IPY_MODEL_3a59fe30158b42989ebb093dd0d7d44b",
      "value": " 6/? [00:03&lt;00:00,  2.06it/s]"
     }
    },
    "9dd37fcca3fc4a6db18785c80e1c5f78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba315b4bee2b4fc89ab9e15b148187fa",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d48e93d6832f4f87a7d2fdaaa275f136",
      "value": 1
     }
    },
    "9e054c86656d4eee8fbc144505d5d679": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e4c8e4c1b8d42c2a03dba54f5c023a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ea1bc73b3444a1a8069231ba5a19f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_88ed841e041443c397df4c060b0824a6",
       "IPY_MODEL_cfc417db2337440bbf2f66abe18afe78",
       "IPY_MODEL_e17b2ac3c7d045b4ac2a5ab21cc289b7"
      ],
      "layout": "IPY_MODEL_414962b849174a3fbfe604637f2b21c3"
     }
    },
    "9f51745451ee4ccbb7749fef279171df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a00412489cfd408aaaf49ab5b9d27ae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d41129dc02ad4648969676e8aa09801b",
       "IPY_MODEL_f708c667e94b428b88d50134b088ef39",
       "IPY_MODEL_58dc1e3b108c448184f4516391d6750c"
      ],
      "layout": "IPY_MODEL_6571bcd10bce453380f6d131bc2dea5e"
     }
    },
    "a0c958294b364af8b07913eeb2645d7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a124de9d40914ac193147bfc2301ca98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75b0dab2d0ee447c97900b821491459c",
      "placeholder": "​",
      "style": "IPY_MODEL_ae6e548c851948219aa7798a5434947e",
      "value": " 9.10M/9.10M [00:00&lt;00:00, 20.0MB/s]"
     }
    },
    "a1cae88538a54a7a9fd225875b5efe12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5fdad66b4534a8289143cc7b1610cfe",
      "placeholder": "​",
      "style": "IPY_MODEL_6ef15d55c12d436d8fce0762c4dd7eca",
      "value": ""
     }
    },
    "a1cb2840011547199cd8ee370d2b3c50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1f613618d1e46749425bd8f1f841162": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a2271b533d6c4262959bac3ed9e56691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58db907bec25432a9cbc04aeec1c205e",
       "IPY_MODEL_d6fe30ebf4864b908c7176fdc114f6b5",
       "IPY_MODEL_e067ade55b824ed48dcf4478e2ff60a5"
      ],
      "layout": "IPY_MODEL_1c6067a81f7a4e1cb7123c5e6b33be4d"
     }
    },
    "a2b750dbdd174d0ca1a292b5d0c831d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a394aae326f84efe97b4c7c1c744f43a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3661a1ad8bbc45a691b4a7c702b5a972",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0682fece9af849e5a74231002cb52a5e",
      "value": 1
     }
    },
    "a3bd3c9988684184b9e8be7b189b269f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f98f87aa5df42f49389fe1830a18528",
      "placeholder": "​",
      "style": "IPY_MODEL_c0acbd7e2f974d1e81b866b3f8372d7b",
      "value": " 13/? [00:04&lt;00:00,  3.45it/s]"
     }
    },
    "a3e2921d5f734b48bb8bda271a827226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a5bad952f6dd4a8e80069cd6184aa42f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a62dafcccde341ddaa23efc711f4baef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6752b077f924a5cb9cc859cb3172c18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d045d76cf5744c99fdbdd3a9f5d278a",
       "IPY_MODEL_eb59193bf7a245c2bb264778ddc41cf1",
       "IPY_MODEL_2d799e14664e4f71a0e52d002603ae6c"
      ],
      "layout": "IPY_MODEL_7c4fc1f6a61d4c25adf706b170972c37"
     }
    },
    "a6bbc24e654240ccbad8d9502d93862b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a6eb1229bf57451797993aae8764b63f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7a4b53ef5524deea80b444f52acb135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7fdfa2ad9b34b739b00741fea65ef60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6eb1229bf57451797993aae8764b63f",
      "placeholder": "​",
      "style": "IPY_MODEL_0fe466b954204ecd9d33caddf248da52",
      "value": ""
     }
    },
    "aa75ea17cff24e78896e72770a75477e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aae08bb8a0df47739cd29b94ca2ff752": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aae3d41ae7c8454181fbd20afcf27940": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aceab5333b98449b80b9b1568ac7aaea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50ec086f5d3f44d98c922ea673db2464",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_71e7bce851454a45ba053634f2a3fdf1",
      "value": 1
     }
    },
    "adca99c9ba6e47e399a9a96d4df13c1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae0a8f582cf3444d9f31daa01d303d46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12263828a3484fd2a40126a2a99096ff",
      "placeholder": "​",
      "style": "IPY_MODEL_b0c4282311394cc8b00954c001b098e5",
      "value": ""
     }
    },
    "ae494952f2aa4bc88a94d92c3527daf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae6e548c851948219aa7798a5434947e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af52104766b44fbbbc9050bd5f040768": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27e6d106679d4fddb3aa31ba458f1087",
      "placeholder": "​",
      "style": "IPY_MODEL_7f414caddd514211ac49b52b58bfd77e",
      "value": ""
     }
    },
    "af5717d5ea944b2796266a43908e3e06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0c4282311394cc8b00954c001b098e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b134bd89f6ea43f1a1f8b67973732c8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b20668b9b7b2442e9b5aff51efb26e5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b36eda2803b9411eb35691db39c59337": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35e4c77a71e54790a57065fb22c5403b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bd0b87187db46aebf0f07e257d0da66",
      "value": 1
     }
    },
    "b481aba88c084a7a96d7964ef18e5df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4e71c6ceb6d4414a0125bf20a871c3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7284b074daf4f32ba02d4d95d905307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b787a54fc0dc4ba08be8a4c98b27b07e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96375517c09545c6802ec8b300699728",
      "placeholder": "​",
      "style": "IPY_MODEL_35cb017075724474ad6746745dbd0b24",
      "value": " 3/? [00:00&lt;00:00,  3.49it/s]"
     }
    },
    "b79225710cee4015894c83d388c3e673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b883757a551f480e8c5ecb911491ec76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b888f5961e444430b6bc3b163bfa1741": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b993d655aa5441f781aa9384b669cd68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c90f659974b4f88be0a45d9bf0fa85e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e540458c64bd42afa3317de48df7458e",
      "value": 1
     }
    },
    "ba315b4bee2b4fc89ab9e15b148187fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "baaf8a7ccc3b4d999eddfe15909216e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3634916312a347fe9e6ee9c1d59b770f",
       "IPY_MODEL_df62ac011e82419299cad74bf6a77146",
       "IPY_MODEL_4cd09071c48d4006aa68bc82dc75d282"
      ],
      "layout": "IPY_MODEL_1d1429d702b5408480f6cdf8094f4b3b"
     }
    },
    "bacc1fb2d502477fb34f90c453c8b4c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fca481b5c1a84ca9a2b4077bfa0788a6",
       "IPY_MODEL_4facfe6fe52546d383fee5342433b6e2",
       "IPY_MODEL_5a42a8d6fb204c0b9e1e7cfd5de6b3ad"
      ],
      "layout": "IPY_MODEL_6f05584129c94c3184735a6e5f7a5e41"
     }
    },
    "bb7d334cf7424f1f87d8a6b3c87416e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc5c0065de9b45f18bfa214c2a470d24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcada27ceac24d758f6fc31a1ecbc944": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcc1a727668e4497a2e87b83d666ba84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcf3045fc06f42dda3236ebd9d00e12c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c62f3b516b840acbf38176e7dea8701",
      "placeholder": "​",
      "style": "IPY_MODEL_08fceb90f22b4db78f6ce0bed8f4e207",
      "value": " 4/? [00:00&lt;00:00,  3.79it/s]"
     }
    },
    "bdebc0d8c5e042e6a2aae324830c233f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_357777cea3fa4abc935f901dbdf1bdf2",
      "placeholder": "​",
      "style": "IPY_MODEL_3d7648a85843493f9f5ffd83161edcf5",
      "value": "Downloading: 100%"
     }
    },
    "be60f1ffaa5c4a28949245b894002faa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be9589537dcf44dd8c85d4b624ace520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c01613112a194d7290eb2ad1736ecc3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_536d0ca8ad3240babc1a747848383813",
      "placeholder": "​",
      "style": "IPY_MODEL_af5717d5ea944b2796266a43908e3e06",
      "value": ""
     }
    },
    "c06189f026484596922c5099d6be1b8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_753482707ba74d16a09305a9e2af2f03",
      "placeholder": "​",
      "style": "IPY_MODEL_822bccb4b6304bbab873c18aac29ddcc",
      "value": "Downloading: 100%"
     }
    },
    "c0acbd7e2f974d1e81b866b3f8372d7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c17032f600a7475facd666bdfe356cbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2b4fd8bac4a74d1bbd244e69678d7aa8",
       "IPY_MODEL_0237d6c8e31b4cb88efc3c554fa503e5",
       "IPY_MODEL_8c4c5e1e60884044bb7df1b115fa7f95"
      ],
      "layout": "IPY_MODEL_74505852eed14afdbab8ef4906af3b1e"
     }
    },
    "c1ed5a249681412aa59a55ea81e4418e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da941db314f94f689096e51697f6c6e5",
       "IPY_MODEL_3b7e34dfd0474b4180277c0260dddca2",
       "IPY_MODEL_12456134939949f7946bb948cc2bd93c"
      ],
      "layout": "IPY_MODEL_3c7fcc0b4def4a198a631e9ea9683a43"
     }
    },
    "c24c279e4c74429f9f0ddbb2f7b4b94b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4b1def8b09d4be99f31ec9c14f84f51",
      "placeholder": "​",
      "style": "IPY_MODEL_096aeabf0e7447edbe88d9861a6d9df8",
      "value": " 15/? [00:09&lt;00:00,  1.89it/s]"
     }
    },
    "c2926566a6464395afcd789a7529d283": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c30a2948dff1476e9551a251a8cad155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4b1def8b09d4be99f31ec9c14f84f51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4b667a28b4c4cb18fa8cfa816853b8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c55cede827941c3aa2b167db6241912",
       "IPY_MODEL_a394aae326f84efe97b4c7c1c744f43a",
       "IPY_MODEL_a3bd3c9988684184b9e8be7b189b269f"
      ],
      "layout": "IPY_MODEL_2448dc0e3d26474a8d474232535c55b4"
     }
    },
    "c4e300a8eb224cb3829d064b88261eea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5b3c53eb92e4e37aaa171b092828eb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5db6170f4bb4461a57de21d67cea869": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05bd42bf76cf4a8bb149287688d320be",
      "placeholder": "​",
      "style": "IPY_MODEL_43bd340459f54bab9b7e030e6244075b",
      "value": "Downloading: 100%"
     }
    },
    "c6aa5349d29840fbaede343ae9c1394d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6db107c2e3946bd89b14bb34ee6b779": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c78bb408f5474dedb9aeb36cf11fe353": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c93c197f924244e1ace188a3b9a4d70d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9a5b71477dc4dde9872bc2812b6ee1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca3b90cf1715458291d9c8aeb99d6083": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db06646b1f4144d2b004e173c89a0b58",
       "IPY_MODEL_7cc9055938dd4a7b80f02be3a0ac9c6e",
       "IPY_MODEL_9362fc344e3b4a1a881a9d7a4718f3d6"
      ],
      "layout": "IPY_MODEL_da47022cb9184c71bb45874bd2964894"
     }
    },
    "ca4167d7ae094dd3b0ff00c283204750": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc004c8f2b3441cd8d395c9f7778dd3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64fc347b02f94171b7cd22a186781e30",
      "placeholder": "​",
      "style": "IPY_MODEL_cd1695bf071644749271f5940d6bca77",
      "value": ""
     }
    },
    "cc5821db3e4147e2bd749c0f8d450b37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ccb370cce221461a819e8e1d822e98a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd1695bf071644749271f5940d6bca77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ceea5f3f020c469693dd241e520ae9bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfc417db2337440bbf2f66abe18afe78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a3312b4fff049528a238aaf225f967f",
      "max": 11551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f281043e69746c58e6a2788f0012c07",
      "value": 11551
     }
    },
    "d031bdcfd5f0452ca1d3104865fda313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6839407e4cdf43159ecf9fa2cff97980",
      "placeholder": "​",
      "style": "IPY_MODEL_88e96bca3f5a4b45a5881234e31df92c",
      "value": "Downloading: 100%"
     }
    },
    "d0a82b71d800496e8ca6caa0f8c6118f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d0f266301e3b466486d74faa83c1f38c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d14309d78a76443c9221a664de5812e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1de70e801584e3bb4e9269d206f8df2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2d10d9522c74359a3cb5739f219864f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af52104766b44fbbbc9050bd5f040768",
       "IPY_MODEL_b993d655aa5441f781aa9384b669cd68",
       "IPY_MODEL_d8bdd0f246684c658d5f5f8b50ea1c4b"
      ],
      "layout": "IPY_MODEL_9e054c86656d4eee8fbc144505d5d679"
     }
    },
    "d41129dc02ad4648969676e8aa09801b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71a477c368284e3eac967eff2f1b5f19",
      "placeholder": "​",
      "style": "IPY_MODEL_0f4d3131dd564abdbc824e4bc9e2928c",
      "value": ""
     }
    },
    "d48e93d6832f4f87a7d2fdaaa275f136": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4c67d0789584630871794cf2ce4f47d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_858dfb6a7a1645328f19574d13f892a0",
       "IPY_MODEL_66cdea98ac614e69b0d1f5f8ac13f951",
       "IPY_MODEL_3ce8029bc0cd4b1890d458ac51d8fc2f"
      ],
      "layout": "IPY_MODEL_a2b750dbdd174d0ca1a292b5d0c831d5"
     }
    },
    "d52799201a11466da64c088f9d23d7ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5680375051b4c06a817a85a5af3a7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6ac3035f4fc46bcbb80f6952a982884": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6fe30ebf4864b908c7176fdc114f6b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f02c2320b514d519b23112088f429c5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41771e47c8484faca97600bc6a8db761",
      "value": 1
     }
    },
    "d736ab979c6f4f52bd79dfac8859ce9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d7926983f7de44259db6cb4d9a9118d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e0bdeb0ab1d4f0284f9ea558cdce38f",
       "IPY_MODEL_1f2c04f83cdd4dcd826f122f2dbd50c7",
       "IPY_MODEL_61e6c0de564344129fb7b4979abc3666"
      ],
      "layout": "IPY_MODEL_77937db7e91549c6b6b8b3812e64e243"
     }
    },
    "d8bdd0f246684c658d5f5f8b50ea1c4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93593049b7a84bd4b802fac3598ff700",
      "placeholder": "​",
      "style": "IPY_MODEL_2b902c865ccf43f0a9e2ae819391a0c2",
      "value": " 14/? [00:08&lt;00:00,  1.93it/s]"
     }
    },
    "d942a6336a0f42f1aac14aabceecd5cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d9a66256a7224d5e8674fbca5703c175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9fea575dbca487b9dd6b04a7bd4143a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f7f72a029344319af2f7e4662d9d9c0",
      "placeholder": "​",
      "style": "IPY_MODEL_641d317f1043472ab3d5c25c62e0ae8f",
      "value": ""
     }
    },
    "da47022cb9184c71bb45874bd2964894": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da941db314f94f689096e51697f6c6e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b0ced42a14c46e7a999cfe4f0d60292",
      "placeholder": "​",
      "style": "IPY_MODEL_55895ac82edb4fe9a24a0a0bc7b4c894",
      "value": ""
     }
    },
    "dabc1eed01e74ea293c943bfb9ad14ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db06646b1f4144d2b004e173c89a0b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb9a1b33045a4f5197da06e6ce6501d6",
      "placeholder": "​",
      "style": "IPY_MODEL_c2926566a6464395afcd789a7529d283",
      "value": ""
     }
    },
    "db4da210dd184d53a2c92a57671c7d79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbc80c4cb096434d87d0bdeaa961bd22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ceea5f3f020c469693dd241e520ae9bf",
      "placeholder": "​",
      "style": "IPY_MODEL_c30a2948dff1476e9551a251a8cad155",
      "value": " 2.24G/2.24G [01:09&lt;00:00, 21.9MB/s]"
     }
    },
    "dd02219e24904db1840d5ca951ad790d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdebc0d8c5e042e6a2aae324830c233f",
       "IPY_MODEL_dda3694e69e542f98bc46eb21edf2498",
       "IPY_MODEL_4448c07bd5ce48c78308283e5960c394"
      ],
      "layout": "IPY_MODEL_b4e71c6ceb6d4414a0125bf20a871c3d"
     }
    },
    "dd29333790d9487d92b482cd0a9fcc60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dd7d373186794a3c9ac329000694c9f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dda3694e69e542f98bc46eb21edf2498": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03ac9a49e4774957b3c603ab4a7b0f5d",
      "max": 513,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_478ad9c8da7544059d3cdcccd0b4040d",
      "value": 513
     }
    },
    "ddd1e0f5b5b14d77be0c0fa9d3dc53de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df62ac011e82419299cad74bf6a77146": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53cc42ffd95b4826ac3591566a81d11e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_671e90a0fffb44449a4c2ee5d93fec8a",
      "value": 1
     }
    },
    "e067ade55b824ed48dcf4478e2ff60a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87ca4a68dee14897b6d7b122ee1bcd8b",
      "placeholder": "​",
      "style": "IPY_MODEL_b481aba88c084a7a96d7964ef18e5df8",
      "value": " 4/? [00:00&lt;00:00,  3.18it/s]"
     }
    },
    "e0a7a546187148b299ef648197f0853f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e17b2ac3c7d045b4ac2a5ab21cc289b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b628b1c30694495b3cb0eeeea09eff6",
      "placeholder": "​",
      "style": "IPY_MODEL_bc5c0065de9b45f18bfa214c2a470d24",
      "value": " 11551/11551 [03:25&lt;00:00, 54.60it/s]"
     }
    },
    "e34c2879a0c94f468c41b7896b237d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f51745451ee4ccbb7749fef279171df",
      "placeholder": "​",
      "style": "IPY_MODEL_d0f266301e3b466486d74faa83c1f38c",
      "value": " 9.10M/9.10M [00:00&lt;00:00, 19.6MB/s]"
     }
    },
    "e36d0e0128d8427b9f7d3fcfac59ff1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3b1e3717522402ab689cf386438700c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd7d373186794a3c9ac329000694c9f8",
      "placeholder": "​",
      "style": "IPY_MODEL_359143f61ed24f8481f13480092c5405",
      "value": " 13/? [00:04&lt;00:00,  3.68it/s]"
     }
    },
    "e411978c571e4f5ba1e99c52829884c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52ac456567384c2da0e97b4daf6a4479",
      "placeholder": "​",
      "style": "IPY_MODEL_104d1062a1b24e29909e86395b7f76f1",
      "value": " 4/? [00:01&lt;00:00,  2.33it/s]"
     }
    },
    "e515382c15a14b4abfe336c0877eaf5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_943cf03c372241d0be5a6d3a4451af6c",
       "IPY_MODEL_1f46d337ae324d17aaca039402af302f",
       "IPY_MODEL_83a10f859d754220b4bb5894ad0ab92f"
      ],
      "layout": "IPY_MODEL_8b0f11d1ec034ec28d792b6b71644d11"
     }
    },
    "e540458c64bd42afa3317de48df7458e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e55f529fc04942109243e8a274c36d56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5fdad66b4534a8289143cc7b1610cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e61231528fe34c3cb842add015f114ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7596b6ec9d0840f6804bd06be12a5f0b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d52799201a11466da64c088f9d23d7ee",
      "value": 1
     }
    },
    "e9029752740946fa9e77b19fcf840dc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e966f67834bf4c5c930f75c3e2f66b56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea48400f02c64d6a94277b81c57e2885": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_259572bb29e34d059192c8b7cf17b4dc",
      "placeholder": "​",
      "style": "IPY_MODEL_0e2dec2610a2414488020a5d1d0ea88a",
      "value": " 4/? [00:02&lt;00:00,  1.53it/s]"
     }
    },
    "eace8a4075764b62889cc33dd1dec4f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb035aff4e47418b88f0f41db8388725": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fcb2693d3214dbb8175394fc0d96e15",
      "placeholder": "​",
      "style": "IPY_MODEL_d5680375051b4c06a817a85a5af3a7ae",
      "value": " 513/513 [00:00&lt;00:00, 12.4kB/s]"
     }
    },
    "eb59193bf7a245c2bb264778ddc41cf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d942a6336a0f42f1aac14aabceecd5cb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47bfd91a954d4313b8439a1251abece7",
      "value": 1
     }
    },
    "ec85a89cfab64e17a7ed792bbe77ad10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed8804207e074fc5a09f9d59cfbfe2fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eed591df093e4080ac746625a0748920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5cda19785cb44507bd3ff1a45f270604",
       "IPY_MODEL_16565a3ef2234725b25eb605e7212c22",
       "IPY_MODEL_b787a54fc0dc4ba08be8a4c98b27b07e"
      ],
      "layout": "IPY_MODEL_93614bdaf09a4d199e88e956a4d442cb"
     }
    },
    "f1e60bf7a6ab4f949965047e901543af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f708c667e94b428b88d50134b088ef39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60e685ab2e6e488f8b9e58a11ffe35f0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a380f3aeff8446581dbee64a6b4609f",
      "value": 1
     }
    },
    "f825d1691d9941b19dc9e3c8f1f09a9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aae08bb8a0df47739cd29b94ca2ff752",
      "max": 2244861551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cb06a9d33b74f8d81d143aa3fe5f35b",
      "value": 2244861551
     }
    },
    "f9e6bc8678d64dc58e01d6a1f5571ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb9a1b33045a4f5197da06e6ce6501d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fca481b5c1a84ca9a2b4077bfa0788a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23ba7435a2df4698b48c88b73327c0d6",
      "placeholder": "​",
      "style": "IPY_MODEL_918f0931a4114ee09c6e30b7317114c0",
      "value": ""
     }
    },
    "fd5c87f273364d6b8b01c7ae0d0d8d60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff4a53d7670a4021a9b8a6179b24a37c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
