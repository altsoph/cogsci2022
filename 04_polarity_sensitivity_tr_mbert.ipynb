{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 20:48:25.464941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "from collections import defaultdict #, Counter\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "MODEL_NAME = 'bert-base-multilingual-uncased'\n",
    "MASK_TOKEN = '[MASK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "BOS_TOKEN = '[CLS]'\n",
    "EOS_TOKEN = '[SEP]'\n",
    "\n",
    "TEST_CASES_PER_SIZE = 100\n",
    "KWORDS_THING = ('hiçbir şey','hiçbir şey','hiçbir şey','hiçbir şey',)\n",
    "KWORDS_BODY  = ('kimseyi','kimseyi','kimseyi','kimseyi',)\n",
    "INPUT_FILE = 'tr_test_sentences.tsv'\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "keywords = list(set(KWORDS_THING)|set(KWORDS_BODY))\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "reverse_vocab = {y:x for x, y in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['kim', '##sey', '##i']\n",
      "2 ['hicbir', 'sey']\n"
     ]
    }
   ],
   "source": [
    "# let's check the number of tokens in our keywords\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "keywords2tokens = dict()\n",
    "# keywords2random_tokens = dict()\n",
    "\n",
    "for keyword in keywords:\n",
    "    tokens = tokenizer.tokenize(keyword)\n",
    "    keywords2tokens[keyword] = tokens\n",
    "#     keywords2random_tokens[keyword] = np.random.choice(range(max(reverse_vocab)), (TEST_CASES_PER_SIZE,len(tokens)))\n",
    "    print(len(tokens), tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse, mask, tokenize\n",
    "\n",
    "subj_dict = dict()\n",
    "verb_dict = dict()\n",
    "\n",
    "def mask_sent(sent, kword):\n",
    "    return [BOS_TOKEN,] + tokenizer.tokenize( \n",
    "        sent.replace(kword, f'{MASK_TOKEN} '*len(keywords2tokens[kword])).replace('  ',' ') \n",
    "    ) + [EOS_TOKEN,]\n",
    "\n",
    "tokenized_array = []\n",
    "metadata_array = []\n",
    "\n",
    "for idx, line in enumerate(open(INPUT_FILE, encoding='utf-8')):\n",
    "    chunks = line.strip().split()\n",
    "    tchunks = line.strip().split('\\t')\n",
    "\n",
    "    if chunks[-1]=='thing':\n",
    "        kwords = KWORDS_THING\n",
    "    else:\n",
    "        kwords = KWORDS_BODY\n",
    "    \n",
    "    subj_id = int(chunks[-3])\n",
    "    subject = chunks[1]\n",
    "    subj_dict[subj_id] = subject\n",
    "    verb_id = int(chunks[-2])\n",
    "    verb1 = chunks[2]\n",
    "    verb2 = chunks[8]\n",
    "    verb_dict[verb_id] = (verb1, verb2)\n",
    "    \n",
    "    for tidx, (cl,kword) in enumerate( zip( ('aff', 'neg', 'many', 'few'), kwords ) ):\n",
    "        mt_sent = mask_sent(tchunks[tidx], kword)\n",
    "        tokenized_array.append( mt_sent )\n",
    "        metadata_array.append( \n",
    "            (\n",
    "                (cl, subj_id, verb_id, kword, chunks[-1]),\n",
    "                (tchunks[tidx], mt_sent)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6379483927243777e-06,\n",
       " 0.00014530157506971106,\n",
       " 5.6702405257912176e-08,\n",
       " 1.9239493708541968e-08,\n",
       " 1.308084678210858e-13,\n",
       " 3.7898307587464684e-13,\n",
       " 1.6448247372619688e-09,\n",
       " 2.235449451130459e-09]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assess_batch(batch, metas):\n",
    "    batch_input_ids = []\n",
    "    batch_segment_ids = []\n",
    "    \n",
    "    mask_positions = []\n",
    "    mask_lens = []\n",
    "\n",
    "    max_len = max(map(len,batch))\n",
    "    \n",
    "    for s in batch:\n",
    "        mask_positions.append( s.index(MASK_TOKEN) )\n",
    "        mask_lens.append( s.count(MASK_TOKEN) )\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(s + [PAD_TOKEN,]*(max_len-len(s)))\n",
    "        batch_input_ids.append( input_ids )\n",
    "        batch_segment_ids.append( [0] * len(input_ids) )\n",
    "\n",
    "    input_ids = torch.tensor(batch_input_ids, dtype=torch.long).to(device)\n",
    "    segment_ids = torch.tensor(batch_segment_ids, dtype=torch.long).to(device)\n",
    "    logits = model(input_ids, token_type_ids=segment_ids)[0]\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    return [\n",
    "        reduce(mul, [pr[pos+t_pos][tokenizer.vocab[tok]].cpu().detach().numpy() \\\n",
    "            for t_pos, tok in enumerate(keywords2tokens[meta[0][3]]) ], 1. ) \\\n",
    "                for pr, pos, meta in zip(probs, mask_positions, metas)\n",
    "    ]\n",
    "        \n",
    "assess_batch(tokenized_array[128*4:130*4], metadata_array[126*4:130*4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00%\n",
      "0.50%\n",
      "1.00%\n",
      "1.50%\n",
      "2.00%\n",
      "2.50%\n",
      "3.00%\n",
      "3.50%\n",
      "4.00%\n",
      "4.50%\n",
      "5.00%\n",
      "5.50%\n",
      "6.00%\n",
      "6.50%\n",
      "7.00%\n",
      "7.50%\n",
      "8.00%\n",
      "8.50%\n",
      "9.00%\n",
      "9.50%\n",
      "10.00%\n",
      "10.50%\n",
      "11.00%\n",
      "11.50%\n",
      "12.00%\n",
      "12.50%\n",
      "13.00%\n",
      "13.50%\n",
      "14.00%\n",
      "14.50%\n",
      "15.00%\n",
      "15.50%\n",
      "16.00%\n",
      "16.50%\n",
      "17.00%\n",
      "17.50%\n",
      "18.00%\n",
      "18.50%\n",
      "19.00%\n",
      "19.50%\n",
      "20.00%\n",
      "20.50%\n",
      "21.00%\n",
      "21.50%\n",
      "22.00%\n",
      "22.50%\n",
      "23.00%\n",
      "23.50%\n",
      "24.00%\n",
      "24.50%\n",
      "25.00%\n",
      "25.50%\n",
      "26.00%\n",
      "26.50%\n",
      "27.00%\n",
      "27.50%\n",
      "28.00%\n",
      "28.50%\n",
      "29.00%\n",
      "29.50%\n",
      "30.00%\n",
      "30.50%\n",
      "31.00%\n",
      "31.50%\n",
      "32.00%\n",
      "32.50%\n",
      "33.00%\n",
      "33.50%\n",
      "34.00%\n",
      "34.50%\n",
      "35.00%\n",
      "35.50%\n",
      "36.00%\n",
      "36.50%\n",
      "37.00%\n",
      "37.50%\n",
      "38.00%\n",
      "38.50%\n",
      "39.00%\n",
      "39.50%\n",
      "40.00%\n",
      "40.50%\n",
      "41.00%\n",
      "41.50%\n",
      "42.00%\n",
      "42.50%\n",
      "43.00%\n",
      "43.50%\n",
      "44.00%\n",
      "44.50%\n",
      "45.00%\n",
      "45.50%\n",
      "46.00%\n",
      "46.50%\n",
      "47.00%\n",
      "47.50%\n",
      "48.00%\n",
      "48.50%\n",
      "49.00%\n",
      "49.50%\n",
      "50.00%\n",
      "50.50%\n",
      "51.00%\n",
      "51.50%\n",
      "52.00%\n",
      "52.50%\n",
      "53.00%\n",
      "53.50%\n",
      "54.00%\n",
      "54.50%\n",
      "55.00%\n",
      "55.50%\n",
      "56.00%\n",
      "56.50%\n",
      "57.00%\n",
      "57.50%\n",
      "58.00%\n",
      "58.50%\n",
      "59.00%\n",
      "59.50%\n",
      "60.00%\n",
      "60.50%\n",
      "61.00%\n",
      "61.50%\n",
      "62.00%\n",
      "62.50%\n",
      "63.00%\n",
      "63.50%\n",
      "64.00%\n",
      "64.50%\n",
      "65.00%\n",
      "65.50%\n",
      "66.00%\n",
      "66.50%\n",
      "67.00%\n",
      "67.50%\n",
      "68.00%\n",
      "68.50%\n",
      "69.00%\n",
      "69.50%\n",
      "70.00%\n",
      "70.50%\n",
      "71.00%\n",
      "71.50%\n",
      "72.00%\n",
      "72.50%\n",
      "73.00%\n",
      "73.50%\n",
      "74.00%\n",
      "74.50%\n",
      "75.00%\n",
      "75.50%\n",
      "76.00%\n",
      "76.50%\n",
      "77.00%\n",
      "77.50%\n",
      "78.00%\n",
      "78.50%\n",
      "79.00%\n",
      "79.50%\n",
      "80.00%\n",
      "80.50%\n",
      "81.00%\n",
      "81.50%\n",
      "82.00%\n",
      "82.50%\n",
      "83.00%\n",
      "83.50%\n",
      "84.00%\n",
      "84.50%\n",
      "85.00%\n",
      "85.50%\n",
      "86.00%\n",
      "86.50%\n",
      "87.00%\n",
      "87.50%\n",
      "88.00%\n",
      "88.50%\n",
      "89.00%\n",
      "89.50%\n",
      "90.00%\n",
      "90.50%\n",
      "91.00%\n",
      "91.50%\n",
      "92.00%\n",
      "92.50%\n",
      "93.00%\n",
      "93.50%\n",
      "94.00%\n",
      "94.50%\n",
      "95.00%\n",
      "95.50%\n",
      "96.00%\n",
      "96.50%\n",
      "97.00%\n",
      "97.50%\n",
      "98.00%\n",
      "98.50%\n",
      "99.00%\n",
      "99.50%\n"
     ]
    }
   ],
   "source": [
    "# assess all the sentences\n",
    "\n",
    "scores = []\n",
    "for idx in range(0, len(tokenized_array), BATCH_SIZE):\n",
    "    if not idx%200: print(f'{idx/(len(tokenized_array)):.2%}')\n",
    "    scores.extend( assess_batch(tokenized_array[idx:idx+BATCH_SIZE], metadata_array[idx:idx+BATCH_SIZE]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(scores).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape scores\n",
    "stats = defaultdict(lambda:defaultdict(lambda:defaultdict(float)))\n",
    "\n",
    "for score, meta in zip(scores, metadata_array):\n",
    "    if meta[0][0]=='aff':\n",
    "        handle = (meta[0][4], 'aff>neg')\n",
    "        k_idx = 0\n",
    "    if meta[0][0]=='neg':\n",
    "        handle = (meta[0][4], 'aff>neg')\n",
    "        k_idx = 1\n",
    "    if meta[0][0]=='many':\n",
    "        handle = (meta[0][4], 'many>few')\n",
    "        k_idx = 0\n",
    "    if meta[0][0]=='few':\n",
    "        handle = (meta[0][4], 'many>few')\n",
    "        k_idx = 1\n",
    "    stats[handle][(meta[0][1], meta[0][2])][k_idx] = score\n",
    "    handle = ('both', handle[1])\n",
    "    stats[handle][(meta[0][1], meta[0][2])][k_idx] += score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('body', 'aff>neg')   21.905%\n",
      "('body', 'many>few')   34.123%\n",
      "('both', 'aff>neg')   18.115%\n",
      "('both', 'many>few')   45.233%\n",
      "('thing', 'aff>neg')   14.542%\n",
      "('thing', 'many>few')   58.588%\n"
     ]
    }
   ],
   "source": [
    "for handle in sorted(stats):\n",
    "    print(handle, \n",
    "          f'  {np.count_nonzero(list(map(lambda x:x[0]>x[1], stats[handle].values())))/len(stats[handle].values()):0.3%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
