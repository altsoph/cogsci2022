### The driving forces of polarity-sensitivity: Experiments with multilingual pre-trained neural language models

*Lisa Bylinina and Aleksey Tikhonov*

This repository containst scripts and data accompanying [our CogSci 2022 paper](https://escholarship.org/uc/item/9xj2t25t).

**Abstract**

Polarity-sensitivity is a typologically general linguistic phenomenon. We focus on negative polarity items (NPIs, e.g. English 'any') -- expressions that are licensed only in negative contexts. The relevant notion of 'negative context' could be defined lexically, syntactically or semantically. There is psycholinguistic evidence in favour of semantics as a driving factor for some NPIs in a couple of languages (Chemla, Homer & Rothschild, 2011; DeniÄ‡, Homer, Rothschild & Chemla, 2021). Testing the scale of this analysis as a potential cross-linguistic universal experimentally is extremely hard. We turn to recent multilingual pre-trained language models -- multilingual BERT (Devlin, Chang, Lee & Toutanova, 2018) and XLM-RoBERTa (Conneau et al., 2019) -- and evaluate the models' recognition of polarity-sensitivity and its cross-lingual generality. Further, using the artificial language learning paradigm, we look for the connection in neural language models between semantic profiles of expressions and their ability to license NPIs. We find evidence for such connection for negation but not for other items we study.

Cite this paper:

```
@inproceedings{bylininatikhonov2022driving,
  title={The driving forces of polarity-sensitivity: Experiments with multilingual pre-trained neural language models},
  author={Bylinina, Lisa and Tikhonov, Alexey},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  year={2022}
}
```
